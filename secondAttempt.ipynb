{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from preprocessing import ReviewPreprocessor\n",
    "from embeddings import EmbeddingsManager\n",
    "from sentiments import Sentiments\n",
    "\n",
    "# Create an instance of classes used in the script\n",
    "sentimentsManager: Sentiments = None\n",
    "embeddingManager: EmbeddingsManager = None\n",
    "preprocessor: ReviewPreprocessor = None\n",
    "\n",
    "# Data are stored into a directory named after the topic and the seed.\n",
    "# e.g., if seed is 1967 and the topic is \"general\", the directory will be \"general/1967\"\n",
    "seed = 1967\n",
    "file_path = \"Magazine_Subscriptions.jsonl\"\n",
    "topicGeneral = os.path.splitext(os.path.basename(file_path))[0]\n",
    "topicPath = os.path.join(\"data\", topicGeneral)\n",
    "if not os.path.exists(topicPath):\n",
    "    os.makedirs(topicPath)\n",
    "# Create a directory for the topic and seed\n",
    "topicSeedPath = os.path.join(topicPath, str(seed))\n",
    "if not os.path.exists(topicSeedPath):\n",
    "    os.makedirs(topicSeedPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d322c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set result file (CSV format, containing original and adjusted ratings)\n",
    "result_file = os.path.join(topicGeneral, f\"{topicGeneral}_results.csv\")\n",
    "\n",
    "# Instantiate the embeddings manager\n",
    "#embeddingManager = EmbeddingsManager(cachePath = topicPath)\n",
    "# Initialize sentiment cache using the instance method\n",
    "sentimentsManager = Sentiments(cachePath=topicPath)\n",
    "# Instantiate the ReviewPreprocessor class (which now handles cache initialization)\n",
    "preprocessor = ReviewPreprocessor(cachePath = topicPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31e7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = \"text\"\n",
    "label_rating = \"rating\"\n",
    "reviewsToProcess: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d089806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1000 reviews.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a random sample of reviews from the file.\n",
    "original_reviews, original_indices = preprocessor.LoadReviews(\n",
    "    file_path, reviewsToProcess, label_text, label_rating, seed\n",
    ")\n",
    "print(f\"\\nLoaded {len(original_reviews)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec5b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing reviews...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preprocessing reviews...\")\n",
    "# Build a dict mapping review text to its rating (this is useful for caching).\n",
    "reviews_dict: dict[str, float] = {\n",
    "    str(review[label_text]): review[label_rating] for review in original_reviews\n",
    "}\n",
    "# Call the class method on the preprocessor instance.\n",
    "preprocessed_reviews = preprocessor.PreprocessReviews(reviews_dict)\n",
    "del original_reviews,original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fe8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analyze_sentiment function\n",
    "def analyze_sentiment(pairs: tuple[str, str],sid = None) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a list of noun/adjective pairs using VADER.\n",
    "    Args:\n",
    "        pairs (list of tuples): List of tuples containing noun/adjective pairs.\n",
    "        sid (SentimentIntensityAnalyzer): Optional VADER sentiment analyzer instance.\n",
    "        If not provided, a new instance will be created.\n",
    "    Returns:\n",
    "        dict: A dictionary with sentiment scores for each pair, using VADER\n",
    "        format (compound, pos, neg, neu).\n",
    "    \"\"\"\n",
    "    if sid is None:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    scores: dict[str, dict] = {}\n",
    "\n",
    "    for noun, adj in pairs:\n",
    "        phrase = f\"{adj} {noun}\"\n",
    "        score = sid.polarity_scores(phrase)\n",
    "        scores[phrase] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c05ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjective-noun pairs\n",
    "pairs: list[tuple[str, str]] = []\n",
    "nouns: list[str] = []\n",
    "# The dictionary associates the original review with its corrected form and\n",
    "# the adjective-noun pairs.\n",
    "reviews_dict: dict[tuple[str,str, str]] = {}\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for rawReview, rawReviewData in preprocessed_reviews.items():\n",
    "\n",
    "    # Check if the data we are about to calculate are already in the cache.\n",
    "    # If so, skip the calculation and use the cached data.\n",
    "    cachedReview = preprocessor.GetReviewFromCache(rawReview)\n",
    "    if cachedReview is not None and \"pairs\" in cachedReview:\n",
    "        # Use the cached data\n",
    "        pairs = cachedReview[\"pairs\"]\n",
    "        nouns = cachedReview[\"nouns\"]\n",
    "    else:\n",
    "        # If not, process the review to extract adjective-noun pairs.\n",
    "        # split sentences on hard punctuation (periods, exclamation marks, question marks)\n",
    "        sentences = re.split(r'(?<=[.!?]) +', rawReviewData[\"corrected\"])\n",
    "        pairs = []\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) < 4:\n",
    "                continue\n",
    "            # Process the sentence with SpaCy.\n",
    "            # This is the core idea of the method: we assume that the sentiment in a review\n",
    "            # is mainly expressed by nouns combined with adjectives, like in \"good music\"\n",
    "            # or \"awful service\"\n",
    "            # The extraction uses Spacy.\n",
    "            # - \"amod\" means adjectival modifier (e.g., \"good\" in \"good music\")\n",
    "            # - \"acomp\" means adjectival complement (e.g., \"good\" in \"the product is good\")\n",
    "            # - \"nsubj\" means nominal subject (e.g., \"product\" in \"the product is good\") \n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    # Token \"children\" are the words that depend on it.\n",
    "                    for child in token.children:\n",
    "                        if child.dep_ == \"amod\":\n",
    "                            # adjective modifier (e.g., \"good\" in \"good music\")\n",
    "                            pairs.append((token.text, child.text))\n",
    "                            nouns.append(token.text)\n",
    "                elif token.dep_ == \"acomp\":\n",
    "                    # adjectival complement (e.g., \"good\" in \"the product is good\").\n",
    "                    # Now search its subject (the noun).\n",
    "                    subjects = [child for child in token.head.children if child.dep_ == \"nsubj\"]\n",
    "                    if subjects:\n",
    "                        # Found, we can add the pair\n",
    "                        pairs.append((subjects[0].text, token.text))\n",
    "                        nouns.append(subjects[0].text)\n",
    "\n",
    "        # Lemmatization is useful for cases where singual and plural forms are used\n",
    "        # interchangeably, like \"good music\" and \"good musics\".\n",
    "        pairs = [(preprocessor.LemmatizeText(noun), adj) for noun, adj in pairs]\n",
    "        # Remove duplicates from pairs\n",
    "        pairs = sorted(list(set(pairs)))\n",
    "        # Recalculate the nouns based on the pairs\n",
    "        nouns = sorted(list(set([noun for noun, _ in pairs])))\n",
    "\n",
    "        # Add the pairs to the preprocessing cache.\n",
    "        # Note the use of item as the key, which is the original review text.\n",
    "#        preprocessor.AddSubitemsToReviewCache(rawReview, {\"pairs\": pairs})\n",
    "#        preprocessor.AddSubitemsToReviewCache(rawReview, {\"nouns\": nouns})\n",
    "\n",
    "    # Add the pairs to the review_dict for later sentiment analysis.\n",
    "    # Differently, the review_dict uses the corrected review text as the key.\n",
    "    reviews_dict[rawReview] = {\n",
    "        \"O-Score\": rawReviewData[\"score\"],\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"nouns\": nouns,\n",
    "        \"pairs\": pairs\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "if 0:\n",
    "    for rawReviewData in reviews_dict.values():\n",
    "        print(f\"Review: {rawReviewData['corrected'][:64]}\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f1dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_reviews_dict: dict[str, list[dict]] = {}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for rawReview, rawReviewData in reviews_dict.items():\n",
    "\n",
    "    pairs = rawReviewData[\"pairs\"]\n",
    "    # Calculate the sentiment scores for the pairs, then filter out\n",
    "    # those with a compound score below 0.05\n",
    "    scores = analyze_sentiment(pairs = pairs, sid = sid)\n",
    "    filtered_pairs = [\n",
    "        (pair.split()[1], pair.split()[0]) \n",
    "        for pair, score in scores.items()\n",
    "        if abs(score['compound']) >= 0.05\n",
    "    ]\n",
    "    V_Pairs: float = np.sum([score['compound'] for score in scores.values()])\n",
    "    # Skip if no pair meets the criteria\n",
    "    if not filtered_pairs:\n",
    "        continue\n",
    "     # Calculate and store:\n",
    "    # - V-whole: the compound score of the review (VADER on the whole review)\n",
    "    # - O-Score: the original score of the review (from the dataset)\n",
    "    V_Whole = sid.polarity_scores(rawReview)[\"compound\"]\n",
    "    O_Score = rawReviewData[\"O-Score\"]\n",
    "\n",
    "    # Add a new key to the filtered_reviews_dict dictionary. We also store\n",
    "    # compound, it will be used later.\n",
    "    filtered_reviews_dict[rawReview] = {\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"pairs\": filtered_pairs,\n",
    "        \"nouns\": sorted(list(set([noun for noun, _ in filtered_pairs]))),\n",
    "        \"V-pairs\": V_Pairs,\n",
    "        \"O-Score\": O_Score,\n",
    "        \"V-whole\": V_Whole\n",
    "    }\n",
    "\n",
    "    # Also update the cache, as the pairs and nouns may have changed.\n",
    "    # We are not interested in storing the scores.\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"pairs\": filtered_pairs})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"nouns\": filtered_reviews_dict[rawReview][\"nouns\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC"
     ]
    }
   ],
   "source": [
    "# In this last step, we invoke a LLM to parse the sentiment score, the so-called \"L-score\".\n",
    "# The LLM will be asked to parse the review text and the noun list, and return a score.\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "parsed_scores: dict[str, dict] = {}\n",
    "\n",
    "# Iterate through each review in the filtered_reviews_dict\n",
    "for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "    # Invoke parseScore() and store the result in the dictionary\n",
    "    parsed_scores[rawReview] = sentimentsManager.parseScore(rawReviewData[\"readable\"], rawReviewData[\"nouns\"])\n",
    "    # Calculate the LLM score as the sum of the parsed scores, then\n",
    "    # add it to the review dictionary.\n",
    "    plusValues = sum([score for score in parsed_scores[rawReview].values() if score > 0])\n",
    "    minusValues = sum([score for score in parsed_scores[rawReview].values() if score < 0])\n",
    "    neutralValues = sum([score for score in parsed_scores[rawReview].values() if score == 0])\n",
    "    llm_score = sum(parsed_scores[rawReview].values())\n",
    "    # Update the review dictionary with the LLM score\n",
    "    rawReviewData[\"L-score\"] = llm_score\n",
    "    rawReviewData[\"L-scoreP\"] = plusValues\n",
    "    rawReviewData[\"L-scoreM\"] = minusValues\n",
    "    rawReviewData[\"L-scoreN\"] = neutralValues\n",
    "    # Add the LLM score to the cache\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-score\": llm_score})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreP\": plusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreM\": minusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreN\": neutralValues})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the filtered results from the new dictionary\n",
    "if 0:\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        print(f\"Review: {rawReview[:32]}...\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n",
    "        print(f\"\\tO-Score (stars): {rawReviewData['O-Score']:.2f}\")\n",
    "        print(f\"\\tL-Score: {rawReviewData['L-score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63e0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the results to a CSV file with adjusted rewiews and all the scores.\n",
    "import csv\n",
    "import time\n",
    "result_file = os.path.join(topicSeedPath, f\"scores.csv\")\n",
    "with open(result_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "    fieldnames = [\n",
    "        'timestamp','O-score',\n",
    "        'L-score','L-scoreP','L-scoreM','L-scoreN',\n",
    "        'V-Whole','readable','corrected','review'\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        writer.writerow({\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'O-score': preprocessed_reviews[rawReview][\"score\"],\n",
    "            'L-score': f\"{rawReviewData['L-score']:.2f}\",\n",
    "            'L-scoreP': f\"{rawReviewData['L-scoreP']:.2f}\",\n",
    "            'L-scoreM': f\"{rawReviewData['L-scoreM']:.2f}\",\n",
    "            'L-scoreN': f\"{rawReviewData['L-scoreN']:.2f}\",\n",
    "            'V-Whole': f\"{rawReviewData['V-whole']:.2f}\",\n",
    "            'readable': f\"{rawReviewData['readable']}\",\n",
    "            'corrected': rawReviewData[\"corrected\"],\n",
    "            'review': rawReview\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b4a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now select reviews and save them to a file for human scoring.\n",
    "import random\n",
    "result_file = os.path.join(topicSeedPath, f\"scores.csv\")\n",
    "# Select up to 100 reviews\n",
    "num_reviews_to_select = min(100, len(filtered_reviews_dict))\n",
    "random.seed(seed)  # Set the seed for reproducibility\n",
    "selected_reviews = random.sample(list(filtered_reviews_dict.items()), num_reviews_to_select)\n",
    "# Save the selected reviews to a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b72df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "selected_reviews_file = os.path.join(topicSeedPath, f\"selected_reviews.csv\")\n",
    "# Check if the file already exists and ask the user if they want to overwrite it\n",
    "if os.path.exists(selected_reviews_file):\n",
    "    overwrite = input(f\"{selected_reviews_file} already exists. Overwrite it? (y/n): \")\n",
    "    if overwrite.lower() != 'y':\n",
    "        print(\"Exiting without overwriting the file.\")\n",
    "        exit()\n",
    "\n",
    "with open(selected_reviews_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['readable','hscore','review']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for rawReview, rawReviewData in selected_reviews:\n",
    "        writer.writerow({\n",
    "            'readable': f\"{rawReviewData['readable']}\",\n",
    "            'hscore': 0,\n",
    "            'review': rawReview\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code stops here. Please manually grade the reviews and run the next script to plot the results.\n"
     ]
    }
   ],
   "source": [
    "print(\"The code stops here. Please manually grade the reviews and run the next script to plot the results.\")\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine human scores and preprocessing cache for ML training\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Read the updated file with human scores\n",
    "with open(selected_reviews_file, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        review = row['review']\n",
    "        hscore = float(row['hscore'])  # Convert human score to float\n",
    "\n",
    "        # Retrieve data from the preprocessing cache\n",
    "        cached_data = preprocessor.GetReviewFromCache(review)\n",
    "        if cached_data:\n",
    "            O_score = cached_data.get(\"score\", 0)\n",
    "            L_scoreP = cached_data.get(\"L-scoreP\", 0)\n",
    "            L_scoreM = cached_data.get(\"L-scoreM\", 0)\n",
    "            L_scoreN = cached_data.get(\"L-scoreN\", 0)\n",
    "\n",
    "            # Append features to X and target to Y\n",
    "            X.append([O_score, L_scoreP, L_scoreM, L_scoreN])\n",
    "            Y.append(hscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e95a45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.15 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Split the data into training and testing sets (85% train, 15% test)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train, X_test, Y_train, Y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Train a Random Forest Regressor\u001b[39;00m\n\u001b[32m      9\u001b[39m model = RandomForestRegressor(random_state=seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giuse\\miniconda3\\envs\\LDA\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giuse\\miniconda3\\envs\\LDA\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giuse\\miniconda3\\envs\\LDA\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.15 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training and testing sets (85% train, 15% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=seed)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32a963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data\\Patio_Lawn_and_Garden\\1967\\ML_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = os.path.join(topicSeedPath, \"ML_model.pkl\")\n",
    "with open(model_file, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375eb5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readable Review: the fountain looks great but the pump stopped working within a couple weeks of me getting the fountain set up and running. i only use it intermittently too so it's not like it was running 24/7\n",
      "Predicted Human Score: 5.72\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select another review from the filtered_reviews_dict\n",
    "\n",
    "random.seed()  # Ensure reproducibility\n",
    "random_review_key = random.choice(list(filtered_reviews_dict.keys()))\n",
    "random_review_data = filtered_reviews_dict[random_review_key]\n",
    "\n",
    "# Extract features for prediction\n",
    "features = [\n",
    "    random_review_data[\"O-Score\"],\n",
    "    random_review_data[\"L-scoreP\"],\n",
    "    random_review_data[\"L-scoreM\"],\n",
    "    random_review_data[\"L-scoreN\"]\n",
    "]\n",
    "\n",
    "# Predict the human score\n",
    "predicted_hscore = model.predict([features])[0]\n",
    "\n",
    "# Print the readable review and the predicted score\n",
    "print(f\"Readable Review: {random_review_data['readable']}\")\n",
    "print(f\"Predicted Human Score: {predicted_hscore:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades: dict[str, int] = {}\n",
    "\n",
    "if 0:\n",
    "    f = open(os.path.join(topicSeedPath, \"grades.txt\"), \"w\", encoding=\"utf-8\")\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        grades[rawReview] = sentimentsManager.assignGradeToReview(rawReviewData[\"readable\"])\n",
    "        f.write(f\"{grades[rawReview]}, \\\"{rawReviewData[\"readable\"]}\\\"\\n\")\n",
    "        f.flush()\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
