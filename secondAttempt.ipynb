{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from preprocessing import ReviewPreprocessor\n",
    "from embeddings import EmbeddingsManager\n",
    "from sentiments import Sentiments\n",
    "\n",
    "# Create an instance of classes used in the script\n",
    "sentimentsManager: Sentiments = None\n",
    "embeddingManager: EmbeddingsManager = None\n",
    "preprocessor: ReviewPreprocessor = None\n",
    "\n",
    "# Data are stored into a directory named after the topic and the seed.\n",
    "# e.g., if seed is 1967 and the topic is \"general\", the directory will be \"general/1967\"\n",
    "seed = 1967\n",
    "file_path = \"Patio_Lawn_and_Garden.jsonl\"\n",
    "topicGeneral = os.path.splitext(os.path.basename(file_path))[0]\n",
    "topicPath = os.path.join(\"data\", topicGeneral)\n",
    "if not os.path.exists(topicPath):\n",
    "    os.makedirs(topicPath)\n",
    "# Create a directory for the topic and seed\n",
    "topicSeedPath = os.path.join(topicPath, str(seed))\n",
    "if not os.path.exists(topicSeedPath):\n",
    "    os.makedirs(topicSeedPath)\n",
    "\n",
    "# scale factors for calculating the final scores. There are two different factor sets:\n",
    "# One is applied to VADER score, the other also consider the LLM analysis on nouns\n",
    "# extracted from noun-adejective pairs above a certain sentiment threshold.\n",
    "\n",
    "# Vader-based Score is calculated with a simple linear equation\n",
    "VScoreMul: float = 2.3\n",
    "VScoreAdd: float = 0.3\n",
    "# R-Score consider both the VADER and the LLM analysis.\n",
    "RScoreVMul: float = 0.95\n",
    "RScoreVAdd: float = 0.4\n",
    "RScoreLMul: float = 0.03\n",
    "RScoreLAdd: float = 1.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d322c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set result file (CSV format, containing original and adjusted ratings)\n",
    "result_file = os.path.join(topicGeneral, f\"{topicGeneral}_results.csv\")\n",
    "\n",
    "# Instantiate the embeddings manager\n",
    "#embeddingManager = EmbeddingsManager(cachePath = topicPath)\n",
    "# Initialize sentiment cache using the instance method\n",
    "sentimentsManager = Sentiments(cachePath=topicPath)\n",
    "# Instantiate the ReviewPreprocessor class (which now handles cache initialization)\n",
    "preprocessor = ReviewPreprocessor(cachePath = topicPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31e7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = \"text\"\n",
    "label_rating = \"rating\"\n",
    "reviewsToProcess: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d089806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1000 reviews.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a random sample of reviews from the file.\n",
    "original_reviews, original_indices = preprocessor.LoadReviews(\n",
    "    file_path, reviewsToProcess, label_text, label_rating, seed\n",
    ")\n",
    "print(f\"\\nLoaded {len(original_reviews)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec5b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing reviews...\n",
      "........."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preprocessing reviews...\")\n",
    "# Build a dict mapping review text to its rating (this is useful for caching).\n",
    "reviews_dict: dict[str, float] = {\n",
    "    str(review[label_text]): review[label_rating] for review in original_reviews\n",
    "}\n",
    "# Call the class method on the preprocessor instance.\n",
    "preprocessed_reviews = preprocessor.PreprocessReviews(reviews_dict)\n",
    "del original_reviews,original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fe8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analyze_sentiment function\n",
    "def analyze_sentiment(pairs: tuple[str, str],sid = None) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a list of noun/adjective pairs using VADER.\n",
    "    Args:\n",
    "        pairs (list of tuples): List of tuples containing noun/adjective pairs.\n",
    "        sid (SentimentIntensityAnalyzer): Optional VADER sentiment analyzer instance.\n",
    "        If not provided, a new instance will be created.\n",
    "    Returns:\n",
    "        dict: A dictionary with sentiment scores for each pair, using VADER\n",
    "        format (compound, pos, neg, neu).\n",
    "    \"\"\"\n",
    "    if sid is None:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    scores: dict[str, dict] = {}\n",
    "\n",
    "    for noun, adj in pairs:\n",
    "        phrase = f\"{adj} {noun}\"\n",
    "        score = sid.polarity_scores(phrase)\n",
    "        scores[phrase] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c05ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjective-noun pairs\n",
    "pairs: list[tuple[str, str]] = []\n",
    "nouns: list[str] = []\n",
    "# The dictionary associates the original review with its corrected form and\n",
    "# the adjective-noun pairs.\n",
    "reviews_dict: dict[tuple[str,str, str]] = {}\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for rawReview, rawReviewData in preprocessed_reviews.items():\n",
    "\n",
    "    # Check if the data we are about to calculate are already in the cache.\n",
    "    # If so, skip the calculation and use the cached data.\n",
    "    cachedReview = preprocessor.GetReviewFromCache(rawReview)\n",
    "    if cachedReview is not None and \"pairs\" in cachedReview:\n",
    "        # Use the cached data\n",
    "        pairs = cachedReview[\"pairs\"]\n",
    "        nouns = cachedReview[\"nouns\"]\n",
    "    else:\n",
    "        # If not, process the review to extract adjective-noun pairs.\n",
    "        # split sentences on hard punctuation (periods, exclamation marks, question marks)\n",
    "        sentences = re.split(r'(?<=[.!?]) +', rawReviewData[\"corrected\"])\n",
    "        pairs = []\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) < 4:\n",
    "                continue\n",
    "            # Process the sentence with SpaCy.\n",
    "            # This is the core idea of the method: we assume that the sentiment in a review\n",
    "            # is mainly expressed by nouns combined with adjectives, like in \"good music\"\n",
    "            # or \"awful service\"\n",
    "            # The extraction uses Spacy.\n",
    "            # - \"amod\" means adjectival modifier (e.g., \"good\" in \"good music\")\n",
    "            # - \"acomp\" means adjectival complement (e.g., \"good\" in \"the product is good\")\n",
    "            # - \"nsubj\" means nominal subject (e.g., \"product\" in \"the product is good\") \n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    # Token \"children\" are the words that depend on it.\n",
    "                    for child in token.children:\n",
    "                        if child.dep_ == \"amod\":\n",
    "                            # adjective modifier (e.g., \"good\" in \"good music\")\n",
    "                            pairs.append((token.text, child.text))\n",
    "                            nouns.append(token.text)\n",
    "                elif token.dep_ == \"acomp\":\n",
    "                    # adjectival complement (e.g., \"good\" in \"the product is good\").\n",
    "                    # Now search its subject (the noun).\n",
    "                    subjects = [child for child in token.head.children if child.dep_ == \"nsubj\"]\n",
    "                    if subjects:\n",
    "                        # Found, we can add the pair\n",
    "                        pairs.append((subjects[0].text, token.text))\n",
    "                        nouns.append(subjects[0].text)\n",
    "\n",
    "        # Lemmatization is useful for cases where singual and plural forms are used\n",
    "        # interchangeably, like \"good music\" and \"good musics\".\n",
    "        pairs = [(preprocessor.LemmatizeText(noun), adj) for noun, adj in pairs]\n",
    "        # Remove duplicates from pairs\n",
    "        pairs = sorted(list(set(pairs)))\n",
    "        # Recalculate the nouns based on the pairs\n",
    "        nouns = sorted(list(set([noun for noun, _ in pairs])))\n",
    "\n",
    "        # Add the pairs to the preprocessing cache.\n",
    "        # Note the use of item as the key, which is the original review text.\n",
    "        preprocessor.AddSubitemsToReviewCache(rawReview, {\"pairs\": pairs})\n",
    "        preprocessor.AddSubitemsToReviewCache(rawReview, {\"nouns\": nouns})\n",
    "\n",
    "    # Add the pairs to the review_dict for later sentiment analysis.\n",
    "    # Differently, the review_dict uses the corrected review text as the key.\n",
    "    reviews_dict[rawReview] = {\n",
    "        \"O-Score\": rawReviewData[\"score\"],\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"nouns\": nouns,\n",
    "        \"pairs\": pairs\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "if 0:\n",
    "    for rawReviewData in reviews_dict.values():\n",
    "        print(f\"Review: {rawReviewData['corrected'][:64]}\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f1dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_reviews_dict: dict[str, list[dict]] = {}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for rawReview, rawReviewData in reviews_dict.items():\n",
    "    pairs = rawReviewData[\"pairs\"]\n",
    "    # Calculate the sentiment scores for the pairs, then filter out\n",
    "    # those with a compound score below 0.05\n",
    "    scores = analyze_sentiment(pairs = pairs, sid = sid)\n",
    "    filtered_pairs = [\n",
    "        (pair.split()[0], pair.split()[1]) \n",
    "        for pair, score in scores.items()\n",
    "        if abs(score['compound']) >= 0.05\n",
    "    ]\n",
    "    V_Pairs: float = np.sum([score['compound'] for score in scores.values()])\n",
    "    # Skip if no pair meets the criteria\n",
    "    if not filtered_pairs:\n",
    "        continue\n",
    "     # Calculate and store:\n",
    "    # - V-whole: the compound score of the review (VADER on the whole review)\n",
    "    # - O-Score: the original score of the review (from the dataset)\n",
    "    V_Whole = sid.polarity_scores(rawReview)[\"compound\"]\n",
    "    O_Score = rawReviewData[\"O-Score\"]\n",
    "\n",
    "    # Add a new key to the filtered_reviews_dict dictionary. We also store\n",
    "    # compound, it will be used later.\n",
    "    filtered_reviews_dict[rawReview] = {\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"pairs\": filtered_pairs,\n",
    "        \"nouns\": sorted(list(set([noun for noun, _ in filtered_pairs]))),\n",
    "        \"V-pairs\": V_Pairs,\n",
    "        \"O-Score\": O_Score,\n",
    "        \"V-whole\": V_Whole\n",
    "    }\n",
    "\n",
    "    # Also update the cache, as the pairs and nouns may have changed.\n",
    "    # We are not interested in storing the scores.\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"pairs\": filtered_pairs})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"nouns\": filtered_reviews_dict[rawReview][\"nouns\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC"
     ]
    }
   ],
   "source": [
    "# In this last step, we invoke a LLM to parse the sentiment score, the so-called \"L-score\".\n",
    "# The LLM will be asked to parse the review text and the noun list, and return a score.\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "parsed_scores: dict[str, dict] = {}\n",
    "\n",
    "# Iterate through each review in the filtered_reviews_dict\n",
    "for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "    # Invoke parseScore() and store the result in the dictionary\n",
    "    parsed_scores[rawReview] = sentimentsManager.parseScore(rawReviewData[\"readable\"], rawReviewData[\"nouns\"])\n",
    "    # Calculate the LLM score as the sum of the parsed scores, then\n",
    "    # add it to the review dictionary.\n",
    "    plusValues = sum([score for score in parsed_scores[rawReview].values() if score > 0])\n",
    "    minusValues = sum([score for score in parsed_scores[rawReview].values() if score < 0])\n",
    "    neutralValues = sum([score for score in parsed_scores[rawReview].values() if score == 0])\n",
    "    llm_score = sum(parsed_scores[rawReview].values())\n",
    "    # Update the review dictionary with the LLM score\n",
    "    rawReviewData[\"L-score\"] = llm_score\n",
    "    rawReviewData[\"L-scoreP\"] = plusValues\n",
    "    rawReviewData[\"L-scoreM\"] = minusValues\n",
    "    rawReviewData[\"L-scoreN\"] = neutralValues\n",
    "    # Add the LLM score to the cache\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-score\": llm_score})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreP\": plusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreM\": minusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreN\": neutralValues})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd4cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have everything to proceed with the final calculation.\n",
    "#\n",
    "# V_Score is obtained from the VADER compound score of the whole review, applying\n",
    "# a linear transformation.\n",
    "# R_Score further corrects the V_Score using the LLM score, which is turn is \n",
    "# obtained from the LLM sentiment analysis of the noun-adjective pairs extracted\n",
    "# from the reviews.\n",
    "#\n",
    "# V_Score = O_Score + (V-whole + 1.0) * 1.5\n",
    "# R_Score = V_Score + (L_score + 0.75) * 0.25\n",
    "\n",
    "for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    " \n",
    "    V_Score: float = rawReviewData[\"O-Score\"] + rawReviewData[\"V-whole\"] * VScoreMul + VScoreAdd\n",
    "    R_Score: float = rawReviewData[\"O-Score\"] + rawReviewData[\"V-whole\"] * RScoreVMul + RScoreVAdd + rawReviewData[\"L-score\"] * RScoreLMul + RScoreLAdd\n",
    "    # Round to the nearest 0.5\n",
    "    V_Score = round(V_Score * 2) / 2\n",
    "    R_Score = round(R_Score * 2) / 2\n",
    "    # Add the scores to the review dictionary\n",
    "    rawReviewData[\"V-score\"] = V_Score\n",
    "    rawReviewData[\"R-score\"] = R_Score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the filtered results from the new dictionary\n",
    "if 0:\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        print(f\"Review: {rawReview[:32]}...\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n",
    "        print(f\"\\tO-Score (stars): {rawReviewData['O-Score']:.2f}\")\n",
    "        print(f\"\\tR-Score: {rawReviewData['R-score']:.2f}\")\n",
    "        print(f\"\\tV-Score: {rawReviewData['V-score']:.2f}\")\n",
    "        print(f\"\\tL-Score: {rawReviewData['L-score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63e0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the results to a CSV file with adjusted rewiews and all the scores.\n",
    "import csv\n",
    "import time\n",
    "result_file = os.path.join(topicSeedPath, f\"scores.csv\")\n",
    "with open(result_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "    fieldnames = [\n",
    "        'timestamp','O-score','R-score','V-score',\n",
    "        'L-score','L-scoreP','L-scoreM','L-scoreN',\n",
    "        'V-Whole','readable','corrected','review'\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        writer.writerow({\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'O-score': preprocessed_reviews[rawReview][\"score\"],\n",
    "            'R-score': f\"{rawReviewData['R-score']:.2f}\",\n",
    "            'V-score': f\"{rawReviewData['V-score']:.2f}\",\n",
    "            'L-score': f\"{rawReviewData['L-score']:.2f}\",\n",
    "            'L-scoreP': f\"{rawReviewData['L-scoreP']:.2f}\",\n",
    "            'L-scoreM': f\"{rawReviewData['L-scoreM']:.2f}\",\n",
    "            'L-scoreN': f\"{rawReviewData['L-scoreN']:.2f}\",\n",
    "            'V-Whole': f\"{rawReviewData['V-whole']:.2f}\",\n",
    "            'readable': f\"{rawReviewData['readable']}\",\n",
    "            'corrected': rawReviewData[\"corrected\"],\n",
    "            'review': rawReview\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b4a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now select reviews and save them to a file for human scoring.\n",
    "import random\n",
    "# Select up to 100 reviews\n",
    "num_reviews_to_select = min(100, len(filtered_reviews_dict))\n",
    "random.seed(seed)  # Set the seed for reproducibility\n",
    "selected_reviews = random.sample(list(filtered_reviews_dict.items()), num_reviews_to_select)\n",
    "# Save the selected reviews to a csv file\n",
    "selected_reviews_file = os.path.join(topicSeedPath, f\"selected_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b72df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(selected_reviews_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['readable','hscore','review']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for rawReview, rawReviewData in selected_reviews:\n",
    "        writer.writerow({\n",
    "            'readable': f\"{rawReviewData['readable']}\",\n",
    "            'hscore': 0,\n",
    "            'review': rawReview\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee9c712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine human scores and preprocessing cache for ML training\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Read the updated file with human scores\n",
    "with open(selected_reviews_file, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        review = row['review']\n",
    "        hscore = float(row['hscore'])  # Convert human score to float\n",
    "\n",
    "        # Retrieve data from the preprocessing cache\n",
    "        cached_data = preprocessor.GetReviewFromCache(review)\n",
    "        if cached_data:\n",
    "            O_score = cached_data.get(\"score\", 0)\n",
    "            L_scoreP = cached_data.get(\"L-scoreP\", 0)\n",
    "            L_scoreM = cached_data.get(\"L-scoreM\", 0)\n",
    "            L_scoreN = cached_data.get(\"L-scoreN\", 0)\n",
    "\n",
    "            # Append features to X and target to Y\n",
    "            X.append([O_score, L_scoreP, L_scoreM, L_scoreN])\n",
    "            Y.append(hscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e95a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.58\n",
      "R^2 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training and testing sets (85% train, 15% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=seed)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32a963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data\\Patio_Lawn_and_Garden\\ML_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = os.path.join(topicSeedPath, \"ML_model.pkl\")\n",
    "with open(model_file, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "375eb5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readable Review: we bought this umbrella stand to use with their blissun umbrella for the pool and it holds up fine when there is no breeze and as long as its standing up straight, but will not hold it up if you need to tilt the umbrella. (umbrella has a tilting mechanism) even standing up straight in a light breeze this thing was easily blowing over. i put two pavers on the base to add weight and that seems to do the trick but still does nothing if you want to tilt it.\n",
      "Predicted Human Score: 5.63\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select another review from the filtered_reviews_dict\n",
    "\n",
    "random.seed()  # Ensure reproducibility\n",
    "random_review_key = random.choice(list(filtered_reviews_dict.keys()))\n",
    "random_review_data = filtered_reviews_dict[random_review_key]\n",
    "\n",
    "# Extract features for prediction\n",
    "features = [\n",
    "    random_review_data[\"O-Score\"],\n",
    "    random_review_data[\"L-scoreP\"],\n",
    "    random_review_data[\"L-scoreM\"],\n",
    "    random_review_data[\"L-scoreN\"]\n",
    "]\n",
    "\n",
    "# Predict the human score\n",
    "predicted_hscore = model.predict([features])[0]\n",
    "\n",
    "# Print the readable review and the predicted score\n",
    "print(f\"Readable Review: {random_review_data['readable']}\")\n",
    "print(f\"Predicted Human Score: {predicted_hscore:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cb881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated file with human scores\n",
    "updated_reviews_file = os.path.join(topicSeedPath, f\"selected_reviews.csv\")\n",
    "with open(updated_reviews_file, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        review = row['review']\n",
    "        hscore = float(row['hscore'])  # Convert hscore to float\n",
    "        # Update the preprocessing cache with the hscore\n",
    "        preprocessor.AddSubitemsToReviewCache(review, {\"hscore\": hscore})\n",
    "        filtered_reviews_dict[review][\"hscore\"] = hscore\n",
    "        # Add to the updated reviews list the original score and the plus, miunus, and zero scores\n",
    "        fullReview, fullReviewData = preprocessor.GetFullReview(review)\n",
    "        updated_reviews_list.append(fu\n",
    "        filtered_reviews_dict[review][\"original_score\"] = row['original_score']\n",
    "        filtered_reviews_dict[review][\"plus_score\"] = row['plus_score']\n",
    "        filtered_reviews_dict[review][\"minus_score\"] = row['minus_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e8cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91661f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaca9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades: dict[str, int] = {}\n",
    "\n",
    "if 0:\n",
    "    f = open(os.path.join(topicSeedPath, \"grades.txt\"), \"w\", encoding=\"utf-8\")\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        grades[rawReview] = sentimentsManager.assignGradeToReview(rawReviewData[\"readable\"])\n",
    "        f.write(f\"{grades[rawReview]}, \\\"{rawReviewData[\"readable\"]}\\\"\\n\")\n",
    "        f.flush()\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
