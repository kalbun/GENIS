{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from preprocessing import ReviewPreprocessor\n",
    "from embeddings import EmbeddingsManager\n",
    "from sentiments import Sentiments\n",
    "\n",
    "# Create an instance of classes used in the script\n",
    "sentimentsManager: Sentiments = None\n",
    "embeddingManager: EmbeddingsManager = None\n",
    "preprocessor: ReviewPreprocessor = None\n",
    "\n",
    "seed = 1967\n",
    "reviewsFileName = \"Software.jsonl\"\n",
    "topicGeneral = os.path.splitext(os.path.basename(reviewsFileName))[0]\n",
    "topicPath = os.path.join(\"data\", topicGeneral)\n",
    "if not os.path.exists(topicPath):\n",
    "    os.makedirs(topicPath)\n",
    "# Create a directory for the topic and seed\n",
    "topicSeedPath = os.path.join(topicPath, str(seed))\n",
    "if not os.path.exists(topicSeedPath):\n",
    "    os.makedirs(topicSeedPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d322c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the embeddings manager\n",
    "#embeddingManager = EmbeddingsManager(cachePath = topicPath)\n",
    "# Initialize sentiment cache using the instance method\n",
    "sentimentsManager = Sentiments(cachePath=topicPath)\n",
    "# Instantiate the ReviewPreprocessor class (which now handles cache initialization)\n",
    "preprocessor = ReviewPreprocessor(cachePath = topicPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31e7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = \"text\"\n",
    "label_rating = \"rating\"\n",
    "reviewsToProcess: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d089806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1000 reviews.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a random sample of reviews from the file.\n",
    "original_reviews, original_indices = preprocessor.LoadReviews(\n",
    "    reviewsFileName, reviewsToProcess, label_text, label_rating, seed\n",
    ")\n",
    "print(f\"\\nLoaded {len(original_reviews)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec5b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing reviews...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preprocessing reviews...\")\n",
    "# Build a dict mapping review text to its rating (this is useful for caching).\n",
    "reviews_dict: dict[str, float] = {\n",
    "    str(review[label_text]): review[label_rating] for review in original_reviews\n",
    "}\n",
    "# Call the class method on the preprocessor instance.\n",
    "preprocessed_reviews = preprocessor.PreprocessReviews(reviews_dict)\n",
    "del original_reviews,original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20aeb38",
   "metadata": {},
   "source": [
    "import shutil\n",
    "\n",
    "# Backup the original JSONL file (note the updated extension)\n",
    "file_path = os.path.join('data', topicGeneral, 'preprocessing_cache.json')\n",
    "backup_file_path = file_path + \".backup\"\n",
    "shutil.copy(file_path, backup_file_path)\n",
    "print(f\"Backup created at {backup_file_path}\")\n",
    "\n",
    "# load the whole file into memory as raw text\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "for review in original_reviews:\n",
    "    abridgedReview = review[label_text][:64]\n",
    "    raw_text = raw_text.replace(review[label_text], abridgedReview)\n",
    "\n",
    "# Write the updated JSON data back to the file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(raw_text)\n",
    "f.close()\n",
    "\n",
    "print(\"Keys replaced and JSONL file updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cda8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fe8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analyze_sentiment function\n",
    "def analyze_sentiment(pairs: tuple[str, str],sid = None) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a list of noun/adjective pairs using VADER.\n",
    "    Args:\n",
    "        pairs (list of tuples): List of tuples containing noun/adjective pairs.\n",
    "        sid (SentimentIntensityAnalyzer): Optional VADER sentiment analyzer instance.\n",
    "        If not provided, a new instance will be created.\n",
    "    Returns:\n",
    "        dict: A dictionary with sentiment scores for each pair, using VADER\n",
    "        format (compound, pos, neg, neu).\n",
    "    \"\"\"\n",
    "    if sid is None:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    scores: dict[str, dict] = {}\n",
    "\n",
    "    for noun, adj in pairs:\n",
    "        phrase = f\"{adj} {noun}\"\n",
    "        score = sid.polarity_scores(phrase)\n",
    "        scores[phrase] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c05ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjective-noun pairs\n",
    "pairs: list[tuple[str, str]] = []\n",
    "nouns: list[str] = []\n",
    "# The dictionary associates the original review with its corrected form and\n",
    "# the adjective-noun pairs.\n",
    "reviews_dict: dict[tuple[str,str, str]] = {}\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for rawReview, rawReviewData in preprocessed_reviews.items():\n",
    "\n",
    "    # Check if the data we are about to calculate are already in the cache.\n",
    "    # If so, skip the calculation.\n",
    "    cachedReview = preprocessor.GetReviewFromCache(rawReview)\n",
    "    if cachedReview is not None and (\"pairs\" in cachedReview) and (\"nouns\" in cachedReview):\n",
    "        pass\n",
    "    else:\n",
    "        # If not, process the review to extract adjective-noun pairs.\n",
    "        # split sentences on hard punctuation (periods, exclamation marks, question marks)\n",
    "        pairs = []\n",
    "        nouns = []\n",
    "        # This regex splits on periods, exclamation marks, and question marks,\n",
    "        sentences = re.split(r'(?<=[.!?]) +', rawReviewData[\"corrected\"])\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) < 4:\n",
    "                continue\n",
    "            # Process the sentence with SpaCy.\n",
    "            # This is the core idea of the method: we assume that the sentiment in a review\n",
    "            # is mainly expressed by nouns combined with adjectives, like in \"good music\"\n",
    "            # or \"awful service\"\n",
    "            # The extraction uses Spacy.\n",
    "            # - \"amod\" means adjectival modifier (e.g., \"good\" in \"good music\")\n",
    "            # - \"acomp\" means adjectival complement (e.g., \"good\" in \"the product is good\")\n",
    "            # - \"nsubj\" means nominal subject (e.g., \"product\" in \"the product is good\") \n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    # Token \"children\" are the words that depend on it.\n",
    "                    for child in token.children:\n",
    "                        if child.dep_ == \"amod\":\n",
    "                            # adjective modifier (e.g., \"good\" in \"good music\")\n",
    "                            pairs.append((token.text, child.text))\n",
    "                            nouns.append(token.text)\n",
    "                elif token.dep_ == \"acomp\":\n",
    "                    # adjectival complement (e.g., \"good\" in \"the product is good\").\n",
    "                    # Now search its subject (the noun).\n",
    "                    subjects = [child for child in token.head.children if child.dep_ == \"nsubj\"]\n",
    "                    if subjects:\n",
    "                        # Found, we can add the pair\n",
    "                        pairs.append((subjects[0].text, token.text))\n",
    "                        nouns.append(subjects[0].text)\n",
    "\n",
    "        # Lemmatization is useful for cases where singual and plural forms are used\n",
    "        # interchangeably, like \"good music\" and \"good musics\".\n",
    "        pairs = [(preprocessor.LemmatizeText(noun), adj) for noun, adj in pairs]\n",
    "        # Remove duplicates from pairs\n",
    "        pairs = sorted(list(set(pairs)))\n",
    "        # Recalculate the nouns based on the pairs\n",
    "        nouns = sorted(list(set([noun for noun, _ in pairs])))\n",
    "\n",
    "    # Add the pairs to the review_dict for later sentiment analysis.\n",
    "    # Differently, the review_dict uses the corrected review text as the key.\n",
    "    reviews_dict[rawReview] = {\n",
    "        \"O-Score\": rawReviewData[\"score\"],\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"nouns\": nouns,\n",
    "        \"pairs\": pairs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f03f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the results\n",
    "if 0:\n",
    "    for rawReviewData in reviews_dict.values():\n",
    "        print(f\"Review: {rawReviewData['corrected'][:64]}\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f1dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating sentiment scores for the reviews...\n",
      "100 of 986\n",
      "200 of 986\n",
      "300 of 986\n",
      "400 of 986\n",
      "500 of 986\n",
      "600 of 986\n",
      "700 of 986\n",
      "800 of 986\n",
      "900 of 986\n",
      "986 of 986 have relevant sentiments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_reviews_dict: dict[str, list[dict]] = {}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "index: int = 0\n",
    "\n",
    "print(\"\\nCalculating sentiment scores for the reviews...\")\n",
    "for rawReview, rawReviewData in reviews_dict.items():\n",
    "\n",
    "    index += 1\n",
    "    if (index % 100) == 0:\n",
    "        print(f\"{index} of {len(reviews_dict)}\")\n",
    "\n",
    "    cachedReview = preprocessor.GetReviewFromCache(rawReview)\n",
    "    if cachedReview is not None:\n",
    "        if \"pairs\" in cachedReview:\n",
    "            # Use the cached data\n",
    "            filtered_pairs = cachedReview[\"pairs\"]\n",
    "        else:\n",
    "            pairs = rawReviewData[\"pairs\"]\n",
    "            # Calculate the sentiment scores for the pairs, then filter out\n",
    "            # those with a compound score below 0.05\n",
    "            scores = analyze_sentiment(pairs = pairs, sid = sid)\n",
    "            filtered_pairs = [\n",
    "                (pair.split()[1], pair.split()[0]) \n",
    "                for pair, score in scores.items()\n",
    "                if abs(score['compound']) >= 0.05\n",
    "            ]\n",
    "            # Skip if no pair meets the criteria\n",
    "            if not filtered_pairs:\n",
    "                continue\n",
    "        if \"nouns\" in cachedReview:\n",
    "            # Use the cached nouns\n",
    "            filtered_nouns = cachedReview[\"nouns\"]\n",
    "        else:\n",
    "            filtered_nouns = sorted(list(set([noun for noun, _ in filtered_pairs])))\n",
    "\n",
    "        # Also update the cache, as the pairs and nouns may have changed.\n",
    "        # We are not interested in storing the scores.\n",
    "        preprocessor.AddSubitemsToReviewCache(rawReview, {\"pairs\": filtered_pairs})\n",
    "        preprocessor.AddSubitemsToReviewCache(rawReview, {\"nouns\": filtered_nouns})\n",
    "\n",
    "    if cachedReview is not None and \"V-Whole\" in cachedReview:\n",
    "        V_Whole = cachedReview[\"V-whole\"]\n",
    "    else:\n",
    "        V_Whole = sid.polarity_scores(rawReview)[\"compound\"]\n",
    "        preprocessor.AddSubitemsToReviewCache(rawReview, {\"V-Whole\": V_Whole}) \n",
    "\n",
    "    # Add a new key to the filtered_reviews_dict dictionary. We also store\n",
    "    # compound, it will be used later.\n",
    "    filtered_reviews_dict[rawReview] = {\n",
    "        \"readable\": rawReviewData[\"readable\"],\n",
    "        \"corrected\": rawReviewData[\"corrected\"],\n",
    "        \"pairs\": filtered_pairs,\n",
    "        \"nouns\": filtered_nouns,\n",
    "        \"O-Score\": rawReviewData[\"O-Score\"],\n",
    "        \"V-whole\": V_Whole,\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"{len(filtered_reviews_dict)} of {len(reviews_dict)} have relevant sentiments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating LLM scores for the reviews...\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC100 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC200 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC300 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC400 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC500 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC600 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC700 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC800 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC900 of 986\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# In this last step, we invoke a LLM to parse the sentiment score, the so-called \"L-score\".\n",
    "# The LLM will be asked to parse the review text and the noun list, and return a score.\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "parsed_scores: dict[str, dict] = {}\n",
    "index: int = 0\n",
    "\n",
    "print(\"\\nCalculating LLM scores for the reviews...\")\n",
    "# Iterate through each review in the filtered_reviews_dict\n",
    "for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "    index += 1\n",
    "    if (index % 100) == 0:\n",
    "        print(f\"{index} of {len(filtered_reviews_dict)}\")\n",
    "    # Invoke parseScore() and store the result in the dictionary\n",
    "    parsed_scores[rawReview] = sentimentsManager.parseScore(rawReviewData[\"readable\"], rawReviewData[\"nouns\"])\n",
    "    # Calculate the LLM score as the sum of the parsed scores, then\n",
    "    # add it to the review dictionary.\n",
    "    plusValues = sum([score for score in parsed_scores[rawReview].values() if score > 0])\n",
    "    minusValues = sum([score for score in parsed_scores[rawReview].values() if score < 0])\n",
    "    neutralValues = sum([score for score in parsed_scores[rawReview].values() if score == 0])\n",
    "    llm_score = sum(parsed_scores[rawReview].values())\n",
    "    # Update the review dictionary with the LLM score\n",
    "    rawReviewData[\"L-score\"] = llm_score\n",
    "    rawReviewData[\"L-scoreP\"] = plusValues\n",
    "    rawReviewData[\"L-scoreM\"] = minusValues\n",
    "    rawReviewData[\"L-scoreN\"] = neutralValues\n",
    "    # Add the LLM score to the cache\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-score\": llm_score})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreP\": plusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreM\": minusValues})\n",
    "    preprocessor.AddSubitemsToReviewCache(rawReview, {\"L-scoreN\": neutralValues})\n",
    "print(\"\\nDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the filtered results from the new dictionary\n",
    "if 0:\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        print(f\"Review: {rawReview[:32]}...\")\n",
    "        print(f\"\\tNouns: {rawReviewData['nouns']}\")\n",
    "        print(f\"\\tPairs: {rawReviewData['pairs']}\")\n",
    "        print(f\"\\tO-Score (stars): {rawReviewData['O-Score']:.2f}\")\n",
    "        print(f\"\\tL-Score: {rawReviewData['L-score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63e0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write the results to a CSV file with adjusted rewiews and all the scores.\n",
    "import csv\n",
    "import time\n",
    "training_file = os.path.join(topicSeedPath, f\"training.csv\")\n",
    "with open(training_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = [\n",
    "        'timestamp','O-score',\n",
    "        'L-score','L-scoreP','L-scoreM','L-scoreN',\n",
    "        'V-Whole','readable','corrected','review'\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        writer.writerow({\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'O-score': preprocessed_reviews[rawReview][\"score\"],\n",
    "            'L-score': f\"{rawReviewData['L-score']:.2f}\",\n",
    "            'L-scoreP': f\"{rawReviewData['L-scoreP']:.2f}\",\n",
    "            'L-scoreM': f\"{rawReviewData['L-scoreM']:.2f}\",\n",
    "            'L-scoreN': f\"{rawReviewData['L-scoreN']:.2f}\",\n",
    "            'V-Whole': f\"{rawReviewData['V-whole']:.2f}\",\n",
    "            'readable': f\"{rawReviewData['readable']}\",\n",
    "            'corrected': rawReviewData[\"corrected\"],\n",
    "            'review': rawReview\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b4a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two CSV files: one for human grade and the other with training data\n",
    "import random\n",
    "import csv\n",
    "result_file = os.path.join(topicSeedPath, f\"scores.csv\")\n",
    "# Select up to 100 reviews\n",
    "num_reviews_to_select = min(100, len(filtered_reviews_dict))\n",
    "random.seed(seed)  # Set the seed for reproducibility\n",
    "selected_reviews = random.sample(list(filtered_reviews_dict.items()), num_reviews_to_select)\n",
    "# Save the selected reviews to a csv file\n",
    "\n",
    "selected_reviews_file = os.path.join(topicSeedPath, f\"selected_reviews.csv\")\n",
    "# Check if the file already exists and ask the user if they want to overwrite it\n",
    "if os.path.exists(selected_reviews_file):\n",
    "    overwrite = input(f\"{selected_reviews_file} already exists. Overwrite it? (y/n): \")\n",
    "    if overwrite.lower() == 'y':\n",
    "        with open(selected_reviews_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['readable','hscore','review']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for rawReview, rawReviewData in selected_reviews:\n",
    "                writer.writerow({\n",
    "                    'readable': f\"{rawReviewData['readable']}\",\n",
    "                    'hscore': 0,\n",
    "                    'review': rawReview\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code stops here. Please manually grade the reviews and run the next script to plot the results.\n"
     ]
    }
   ],
   "source": [
    "print(\"The code stops here. Please manually grade the reviews and run the next script to plot the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee9c712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Combine human scores and preprocessing cache for ML training\n",
    "Reviews: list[str] = []\n",
    "X: list[list[float,float,float,float]] = []\n",
    "Y: list[float] = []\n",
    "V_scores: list[float] = []\n",
    "# Read the updated file with human scores\n",
    "selected_reviews_file = os.path.join(topicSeedPath, f\"selected_reviews.csv\")\n",
    "with open(selected_reviews_file, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        review = row['review']\n",
    "        hscore = float(row['hscore'])  # Convert human score to float\n",
    "\n",
    "        # Retrieve data from the preprocessing cache\n",
    "        cached_data = preprocessor.GetReviewFromCache(review)\n",
    "        if cached_data:\n",
    "            O_score = cached_data.get(\"score\", 0)\n",
    "            L_scoreP = cached_data.get(\"L-scoreP\", 0)\n",
    "            L_scoreM = cached_data.get(\"L-scoreM\", 0)\n",
    "            L_scoreN = cached_data.get(\"L-scoreN\", 0)\n",
    "            V_score = cached_data.get(\"V-whole\", 0)\n",
    "\n",
    "            # Append features to X and target to Y\n",
    "            X.append([O_score, L_scoreP, L_scoreM, L_scoreN])\n",
    "            Y.append(hscore)\n",
    "            Reviews.append(cached_data.get(\"readable\", \"\"))\n",
    "            V_scores.append(V_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e95a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.34\n",
      "R^2 Score: 0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test, Rev_train, Rev_test, V_train, V_test = train_test_split(X, Y, Reviews, V_scores, test_size=0.20, random_state=seed)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=seed)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92e8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data\\Patio_Lawn_and_Garden\\1967\\results.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(topicSeedPath, \"results.csv\")\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Define the fieldnames for the CSV\n",
    "    fieldnames = ['timestamp', 'type', 'O-score', 'L-scoreP', 'L-scoreM', 'L-scoreN', 'Y', 'Y-pred', 'VADER', 'reviewfile', 'review']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write training data\n",
    "    for i in range(len(X_train)):\n",
    "        writer.writerow({\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'type': 'tr',\n",
    "            'O-score': X_train[i][0],\n",
    "            'L-scoreP': X_train[i][1],\n",
    "            'L-scoreM': X_train[i][2],\n",
    "            'L-scoreN': X_train[i][3],\n",
    "            'Y': Y_train[i],\n",
    "            'Y-pred': Y_train[i],  # For training data, Y-pred is the same as Y\n",
    "            'VADER': V_train[i],  # Add the VADER score for training data\n",
    "            'reviewfile': reviewsFileName,\n",
    "            'review': Rev_train[i],  # Add the review text for training data\n",
    "        })\n",
    "    \n",
    "    # Write test data\n",
    "    for i in range(len(X_test)):\n",
    "        writer.writerow({\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'type': 'te',\n",
    "            'O-score': X_test[i][0],\n",
    "            'L-scoreP': X_test[i][1],\n",
    "            'L-scoreM': X_test[i][2],\n",
    "            'L-scoreN': X_test[i][3],\n",
    "            'Y': Y_test[i],\n",
    "            'Y-pred': f\"{Y_pred[i]:.2f}\",  # For test data, use the predicted Y\n",
    "            'VADER': V_test[i],  # Add the VADER score for test data\n",
    "            'reviewfile': reviewsFileName,\n",
    "            'review': Rev_test[i],  # Add the review text for test data\n",
    "        })\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e32a963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data\\Patio_Lawn_and_Garden\\1967\\ML_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = os.path.join(topicSeedPath, \"ML_model.pkl\")\n",
    "with open(model_file, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades: dict[str, int] = {}\n",
    "\n",
    "if 0:\n",
    "    f = open(os.path.join(topicSeedPath, \"grades.txt\"), \"w\", encoding=\"utf-8\")\n",
    "    for rawReview, rawReviewData in filtered_reviews_dict.items():\n",
    "        grades[rawReview] = sentimentsManager.assignGradeToReview(rawReviewData[\"readable\"])\n",
    "        f.write(f\"{grades[rawReview]}, \\\"{rawReviewData[\"readable\"]}\\\"\\n\")\n",
    "        f.flush()\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
