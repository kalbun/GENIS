#
# GENIS - Grading with Explainability and No Intrinsic Saturation
#
# genisTrain.py - Script to train a linear regression model for sentiment analysis
# using features extracted from reviews.
# Before running this script, you should execute genisCalc.py to get the data needed.
#
import csv
import os
import argparse
import datetime
import pickle
from sklearn.model_selection import train_test_split
from scipy.stats import pearsonr
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, confusion_matrix
from preprocessing import ReviewPreprocessor
import matplotlib.pyplot as plt

ver: str = "0.10.0"
# Labels for the text and rating in the jsonl file
# The default values are the ones used in the Amazon reviews dataset
label_text: str = "text"
label_rating: str = "rating"

# Create an instance of class used in the script.
# Initialization postponed as it requires the cache path, calculated later.
preprocessor: ReviewPreprocessor = None

# Dictionary to store the data acquired from the csv files and the caches
DataDict: dict = {}
# Lists to store the data acquired from the csv files and the caches
Reviews: list[str] = []
X_train: list[list[float]] = []
X_test: list[list[float]] = []
Y: list[float] = []
Y_train: list[float] = []
Y_test: list[float] = []
V_scores: list[float] = []
LLM_scores: list[float] = []

helpMessage = """\
GENIS - Grading with Explainability and No Intrinsic Saturation

This script trains a linear regression model for sentiment analysis using features extracted from reviews.
To run, genisTrain requires a data file called selected_reviews.csv and the preprocessing cache generated by genisCalc.py.
Moreover, selected_reviews.csv must be modified beforehand to include the human scores for the reviews.

To locate these files, genisTrain uses two command line parameters:

- a positional parameter with the subdirectory of 'data/' where files to process are stored.
- the optional parameter -s or --seed is the random seed used with genisCalc.py (default 1967)

For example:

    python genisTrain.py music -s 42

This will look for the cache in data/music and the csv in data/music/42/selected_reviews.csv.
"""

print(f"GENIS trainer v{ver}")
parser = argparse.ArgumentParser(
    description=helpMessage,
    formatter_class=argparse.RawTextHelpFormatter
)
parser.add_argument(
    "paths",
    type=str,
    nargs="+",  # Accept one or more directories
    help="List of directories under 'data/' where processed files are stored"
)
parser.add_argument("-s", "--seed", type=int, help="Random seed (default 1967)", default=1967)
parser.add_argument("-v", "--version", action="version", version=f"{ver}")
args = parser.parse_args()

# Process command line arguments

for path in args.paths:

    cachePath = os.path.join("data", path)
    filePath = os.path.join(cachePath, f"{args.seed}", "selected_reviews.csv")
    if not os.path.exists(cachePath):
        print(f"Directory data/{path} not found.")
        exit(1)
    if not os.path.exists(filePath):
        print(f"File {filePath} not found.")
        exit(1)

    # Instantiate the ReviewPreprocessor class (which now handles cache initialization)
    preprocessor = ReviewPreprocessor(cachePath = cachePath)

    # Read the updated file with human scores
    counter: int = 0
    with open(filePath, mode='r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            review = row['review']
            hscore = float(row['hscore'])  # Convert human score to float

            # Retrieve data from the preprocessing cache
            cached_data = preprocessor.GetReviewFromCache(review)
            if cached_data:
                O_score = cached_data.get("score", 0)
                parsed_scores = cached_data.get("parsed_scores", {})
                # Extract the scores from the parsed scores dictionary
                DataDict[review] = {
                    "O-score": O_score,
                    "L-scoreP": sum([score for score in parsed_scores.values() if score > 0]),
                    "L-scoreM": sum([score for score in parsed_scores.values() if score < 0]),
                    "L-scoreN": sum([score for score in parsed_scores.values() if score == 0]),
                    "hscore": str(hscore),
                    "V-Whole": cached_data.get("V-Whole", 0),
                    "LLM-score": cached_data.get("LLM-score", 0),
                    "readable": cached_data.get("readable", "")
                }
                counter += 1

    print(f"Loaded {counter} reviews from {filePath}")

# Split the data into training and testing sets (80% train, 20% test)
DataDict_train, DataDict_test = train_test_split(list(DataDict.values()), test_size=0.20, random_state=args.seed)

Y = [data["hscore"] for data in DataDict.values()]
X_train = [
    [data["O-score"], data["L-scoreP"], data["L-scoreM"], data["L-scoreN"]]
    for data in DataDict_train
    ]
X_test = [
    [data["O-score"], data["L-scoreP"], data["L-scoreM"], data["L-scoreN"]]
    for data in DataDict_test
    ]
Y_train = [data["hscore"] for data in DataDict_train]
Y_test = [data["hscore"] for data in DataDict_test]
V_scores = [data["V-Whole"] for data in DataDict.values()]
V_scores = [(int( ((v + 1) * 3.5 + 1) * 2) / 2) for v in V_scores]  # Adjust VADER scores
V_labels = [str(v) for v in V_scores]
LLM_scores = [data["LLM-score"] for data in DataDict.values()]
LLM_labels = [ str(l) for l in LLM_scores]  # Adjust LLM scores to match the labels

model = RandomForestClassifier(n_estimators=16, min_samples_split=5, random_state=args.seed)
print(f"Training model with {len(X_train)} samples")
model.fit(X_train, Y_train)
# Make predictions on the test set
print(f"Predicting {len(X_test)} samples")
Y_pred = model.predict(X_test)

# Evaluate the model
print("Evaluation metrics, F1 with human baseline:")
print("Params\tweight\tmicro\tmacro\tPearson")
print(f"Train\t{f1_score(Y_train, model.predict(X_train), average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='micro'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y_train], [float(y) for y in model.predict(X_train)])[0]:.2f}")
print(f"Test\t{f1_score(Y_test, Y_pred, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_test, Y_pred, average='micro'):.2f}",end="")  
print(f"\t{f1_score(Y_test, Y_pred, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y_test], [float(y) for y in Y_pred])[0]:.2f}")
print(f"VADER\t{f1_score(Y, V_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y, V_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y, V_labels, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y], [float(y) for y in V_scores])[0]:.2f}\t(whole sample)")
print(f"LLM\t{f1_score(Y,LLM_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y, LLM_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y, LLM_labels, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y], [float(y) for y in LLM_scores])[0]:.2f}\t(whole sample)")

# Show the confusion matrices
plt.figure(figsize=(10, 7))
plt.subplot(1, 2, 1)
# Calculate ordered labels from numeric values. This quite complex operation is
# needed to show the labels in numerical order.
# Only get the labels from the test set
LLM_labelsT = [str(l) for l in [data["LLM-score"] for data in DataDict_test]]
ordered_labels = [str(v) for v in sorted([float(s) for s in (set(Y_test + LLM_labelsT))])]
# First confusion matrix (LLM vs Y_test)
confusion_matrix_result = confusion_matrix(
    y_true=Y_test,
    y_pred=LLM_labelsT,
    labels=ordered_labels
)
plt.imshow(confusion_matrix_result, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion matrix for LLM")
plt.colorbar()
tick_marks = range(len(ordered_labels))
plt.xticks(tick_marks, ordered_labels, rotation=45)
plt.yticks(tick_marks, ordered_labels)
plt.xlabel("LLM")
plt.ylabel("Human")
# Annotate the cells with the confusion matrix values
for i in range(confusion_matrix_result.shape[0]):
    for j in range(confusion_matrix_result.shape[1]):
        plt.text(j, i, format(confusion_matrix_result[i, j], 'd'),
                 ha="center", va="center", color="black")

plt.subplot(1, 2, 2)
# Second confusion matrix (Y_pred vs Y_test)
ordered_labels = [str(v) for v in sorted([float(s) for s in (set(Y_test + Y_test))])]
confusion_matrix_result = confusion_matrix(
    y_true=Y_test,
    y_pred=Y_pred,
    labels=ordered_labels
)
plt.imshow(confusion_matrix_result, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion matrix for GENIS")
plt.colorbar()
tick_marks = range(len(ordered_labels))
plt.xticks(tick_marks, ordered_labels, rotation=45)
plt.yticks(tick_marks, ordered_labels)
plt.xlabel("GENIS")
plt.ylabel("Human")
# Annotate the cells with the confusion matrix values
for i in range(confusion_matrix_result.shape[0]):
    for j in range(confusion_matrix_result.shape[1]):
        plt.text(j, i, format(confusion_matrix_result[i, j], 'd'),
                 ha="center", va="center", color="black")

plt.tight_layout()
plt.show()

# Write the contents of all files into a new CSV file
with open('data/overall_results.csv', 'w', encoding='utf-8', newline='') as file:
    writer = csv.writer(file)
    # Write the header
    writer.writerow([
        "timestamp",  # timestamp
        "type",  # type (train/test)
        "O-score",  # O-score
        "L-scoreP",  # L-scoreP
        "L-scoreM",  # L-scoreM
        "L-scoreN",  # L-scoreN
        "Y",  # Y (human score)
        "Y-pred",  # Y-pred (to be filled later)
        "VADER",  # VADER score
        "LLM",  # LLM score
        "review"  # review file
    ])
    for data in DataDict_train:
        writer.writerow([
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # timestamp
            "tr",  # type (train/test)
            data["O-score"],  # O-score
            data["L-scoreP"],  # L-scoreP
            data["L-scoreM"],  # L-scoreM
            data["L-scoreN"],  # L-scoreN
            float(data["hscore"]),  # Y (human score)
            float(data["hscore"]),  # Y-pred (same as Y for training data)
            data["V-Whole"],  # VADER score
            data["LLM-score"],  # LLM score
            data["readable"]  # review file
        ])
    for data in DataDict_test:
        writer.writerow([
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # timestamp
            "te",  # type (train/test)
            data["O-score"],  # O-score
            data["L-scoreP"],  # L-scoreP
            data["L-scoreM"],  # L-scoreM
            data["L-scoreN"],  # L-scoreN
            float(data["hscore"]),  # Y (human score)
            float(data["hscore"]),  # Y-pred (to be filled later)
            data["V-Whole"],  # VADER score
            data["LLM-score"],  # LLM score
            data["readable"]  # review file
        ])

file.close()
print("Results written to data/overall_results.csv")

# Save the model to a file
with open('data/random_forest_classifier.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
print("Model dumped successfully in data/random_forest_classifier.pkl (pickle format).")
print("Done.")

