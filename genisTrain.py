#
# GENIS - Grading with Explainability and No Intrinsic Saturation
#
# genisTrain.py - Script to train a linear regression model for sentiment analysis
# using features extracted from reviews.
# Before running this script, you should execute genisCalc.py to get the data needed.
#
import csv
import os
import argparse
import datetime
import pickle
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr, pearsonr
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
import sklearn.metrics
from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score
from preprocessing import ReviewPreprocessor
import matplotlib.pyplot as plt
import matplotlib.patches as patches

ver: str = "0.15.0"
# Labels for the text and rating in the jsonl file
# The default values are the ones used in the Amazon reviews dataset
label_text: str = "text"
label_rating: str = "rating"

# Create an instance of class used in the script.
# Initialization postponed as it requires the cache path, calculated later.
preprocessor: ReviewPreprocessor

# Dictionary to store the data acquired from the csv files and the caches
DataDict: dict = {}
# Lists to store the data acquired from the csv files and the caches
Reviews: list[str] = []
X_train: list[list[float]] = []
X_test: list[list[float]] = []
Y: list[float] = []
Y_train: list[float] = []
Y_test: list[float] = []
V_scores: list[float] = []
LLM_scores: list[float] = []

helpMessage = """\
GENIS - Grading with Explainability and No Intrinsic Saturation

This script trains a linear regression model for sentiment analysis using features extracted from reviews.
To run, genisTrain requires a data file called selected_reviews.csv and the preprocessing cache generated by genisCalc.py.
Moreover, selected_reviews.csv must be modified beforehand to include the human scores for the reviews.

To locate these files, genisTrain uses two command line parameters:

- a positional parameter with the subdirectory of 'data/' where files to process are stored.
- the optional parameter -s or --seed is the random seed used with genisCalc.py (default 1967)

For example:

    python genisTrain.py music -s 42

This will look for the cache in data/music and the csv in data/music/42/selected_reviews.csv.
"""

def writeCSV(train_data, test_data, filename):
    """Write the overall results CSV from training and test data dictionaries."""
    with open(filename, 'w', encoding='utf-8', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([
            "timestamp", "type", "O-score", "G-scoreP", "G-scoreM", "G-scoreN", "Y", "Y-pred", "V-converted", "LLM", "VADER", "review", "filename"
        ])
        for data in train_data:
            writer.writerow([
                datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "tr",
                data["O-score"],
                data["G-scoreP"],
                data["G-scoreM"],
                data["G-scoreN"],
                float(data["Y"]),
                float(data["Y"]),
                data["V-converted"],
                data["LLM-score"],
                data["VADER"],
                data["readable"],
                data["filename"]
            ])
        for data in test_data:
            writer.writerow([
                datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "te",
                data["O-score"],
                data["G-scoreP"],
                data["G-scoreM"],
                data["G-scoreN"],
                float(data["Y"]),
                float(data["Y-pred"]),
                data["V-converted"],
                data["LLM-score"],
                data["VADER"],
                data["readable"],
                data["filename"]
            ])
    print(f"Results written to {filename}")

def loadCSV(filename: str) -> dict:
    """Load the overall results CSV and return lists for each column."""
    timestamps, types, O_scores, G_scorePs, G_scoreMs, G_scoreNs = [], [], [], [], [], []
    Ys, Y_preds, V_converted, LLMs, VADERs, reviews, filenames = [], [], [], [], [], [], []

    dictionary: dict[str, dict] = {}

    with open(filename, 'r', encoding='utf-8') as file:
        fieldnames = [
            "timestamp", "type", "O-score", "G-scoreP", "G-scoreM", "G-scoreN", "Y", "Y-pred", "V-converted", "LLM", "VADER", "review", "filename"
        ]
        # create a reader. The first line is the header
        # and the rest is the data
        reader = csv.DictReader(file, fieldnames=fieldnames)
        reader.__next__()  # Skip the header
        for row in reader:
            # Append each row to the corresponding list
            # Note that the LM model is a classifier, so we need to convert
            # the score to a string to match the labels
            dictionary[row["review"]] = {
                "timestamp": row["timestamp"],
                "type": row["type"],
                "O-score": float(row["O-score"]),
                "G-scoreP": float(row["G-scoreP"]),
                "G-scoreM": float(row["G-scoreM"]),
                "G-scoreN": float(row["G-scoreN"]),
                "Y": str(row["Y"]),
                "Y-pred": str(row["Y-pred"]),
                "V-converted": str(row["V-converted"]),
                "LLM-score": str(row["LLM"]),
                "VADER": float(row["VADER"]),
                "review": row["review"],
                "filename": row["filename"]
            }
        return dictionary

def plotConfusionMatrix(y_true: list, y_pred: list, labels: list, title: str, xlabel: str, ylabel: str):
    """Plot the confusion matrix."""
    cm = confusion_matrix(y_true, y_pred, labels=labels)
#    plt.imshow(cm, cmap=plt.cm.Grays)
#    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Grays)
#    plt.colorbar()
    # Set the title and labels
    plt.title(title)
    tick_marks = range(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.xlabel(xlabel=xlabel)
    plt.ylabel(ylabel=ylabel)
    # Annotate the cells with the confusion matrix values
    max_value = cm.max()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            if (i == j and i >= 1 and i < 20):
#                plt.gca().add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='lime', lw=0.5))
                plt.gca().add_patch(patches.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, color='lime', alpha=0.3))
            if cm[i, j] != 0:
                if (i != j):
                    plt.gca().add_patch(patches.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, color='red', alpha=0.2))
                plt.text(j, i, format(cm[i, j], 'd'),ha="center", va="center", color='black', fontsize=10)
                # also set background color


DataDict: dict = {}

print(f"GENIS trainer v{ver}")
parser = argparse.ArgumentParser(
    description=helpMessage,
    formatter_class=argparse.RawTextHelpFormatter
)
parser.add_argument(
    "paths",
    type=str,
    nargs="*",  # Accept zero or more directories
    help="List of directories under 'data/' where processed files are stored"
)
parser.add_argument("-s", "--seed", type=int, help="Random seed (default 1967)", default=1967)
parser.add_argument("-v", "--version", action="version", version=f"{ver}")
parser.add_argument("-i", "--image", action="store_true", help="Generate confusion matrix images")
parser.add_argument('-l', '--load', type=str, default=None, help="Load data from a CSV file instead of reading the paths")
parser.add_argument('-f', '--force', action='store_true', help="Force overwrite of existing files")
args = parser.parse_args()

# Process command line arguments

if (args.load is not None):
    # Load data from the specified CSV file
    if not os.path.exists(args.load):
        print(f"File {args.load} not found.")
        exit(1)
    # Load the CSV file into DataDict
    DataDict = loadCSV(args.load)
    print(f"Loaded {len(DataDict)} reviews from {args.load}")

elif len(args.paths) != 0:

    for path in args.paths:

        cachePath = os.path.join("data", path)
        filePath = os.path.join(cachePath, f"{args.seed}", "selected_reviews.csv")
        if not os.path.exists(cachePath):
            print(f"Directory data/{path} not found.")
            exit(1)
        if not os.path.exists(filePath):
            print(f"File {filePath} not found.")
            exit(1)

        # Instantiate the ReviewPreprocessor class (which now handles cache initialization)
        preprocessor = ReviewPreprocessor(cachePath = cachePath)

        # Read the updated file with human scores
        counter: int = 0
        with open(filePath, mode='r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                review = row['review']
                y_score = float(row['hscore'])  # Convert human score to float

                # Retrieve data from the preprocessing cache
                cached_data = preprocessor.GetReviewFromCache(review)
                if cached_data:
                    O_score = cached_data.get("score", 0)
                    parsed_scores = cached_data.get("parsed_scores", {})
                    # Extract the scores from the parsed scores dictionary
                    DataDict[review] = {
                        "O-score": O_score,
                        "G-scoreP": sum([score for score in parsed_scores.values() if score > 0]),
                        "G-scoreM": sum([score for score in parsed_scores.values() if score < 0]),
                        "G-scoreN": sum([score for score in parsed_scores.values() if score == 0]),
                        "Y": str(y_score),
                        "Y-pred": str(y_score),  # Placeholder for predictions
                        "VADER": cached_data.get("V-Whole", 0),
                        # convert VADER score to a 1-10 scale with half grades
                        "V-converted": round( ((cached_data.get("V-Whole", 0) + 1) * 4.5 + 1) * 2, 0) / 2,
                        "LLM-score": cached_data.get("LLM-score", 0),
                        "readable": cached_data.get("readable", ""),
                        "filename": path
                    }
                    counter += 1

        print(f"Loaded {counter} reviews from {filePath}")

else:
    print("Please provide a list of directories or use the -l option to load data from a CSV file.")
    exit(1)

# At this point, DataDict contains all the data needed for training, obtained
# either from the paths provided or from the CSV file.

# Filter out reviews with no scores
#DataDict = {key: values for key, values in DataDict.items() if (values["G-scoreP"] + values["G-scoreM"] + values["G-scoreN"]) > 0}

# Split the data into training and testing sets (80% train, 20% test)
DataDict_train, DataDict_test = train_test_split(list(DataDict.values()), test_size=0.2, random_state=args.seed)

Y = [data["Y"] for data in DataDict.values()]
X_train = [
    [data["O-score"], data["G-scoreP"], data["G-scoreM"], data["G-scoreN"]
     ]
    for data in DataDict_train
    ]
X_test = [
    [data["O-score"], data["G-scoreP"], data["G-scoreM"], data["G-scoreN"]
     ]
    for data in DataDict_test
    ]
Y_train = [data["Y"] for data in DataDict_train]
Y_test = [data["Y"] for data in DataDict_test]
# Prepare baseline scores/labels using the test split (so baselines are comparable to model test preds)
V_scores = [data["V-converted"] for data in DataDict_test]
V_labels = [str(v) for v in V_scores]
LLM_scores = [data["LLM-score"] for data in DataDict_test]
LLM_labels = [str(l) for l in LLM_scores]

modelRF = RandomForestClassifier(
    n_estimators=128,
    criterion='log_loss',
    max_depth=None,
    max_leaf_nodes=15,
    random_state=args.seed)

modelDT = DecisionTreeClassifier(
    criterion='gini',
    max_depth=10,
    min_samples_split=2,
    max_leaf_nodes=20,
    min_samples_leaf=1,
    random_state=args.seed)

modelNB = GaussianNB()

"""
# Random forest calibration

from sklearn.model_selection import cross_val_score, KFold, GridSearchCV
from sklearn.ensemble import RandomForest
import numpy as np


X = X_train
y = Y_train
# Convert the lists to numpy arrays for compatibility with scikit-learn
X = np.array(X)
y = np.array(y)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [x for x in range(48, 129, 8)],
    'criterion': ['gini','log_loss'],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 4],
    'max_leaf_nodes': [None, 15, 20, 25],
    'min_samples_leaf': [1, 2],
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Perform grid search
grid_search.fit(X, y)

# Print the best parameters and the best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# Initialize k-fold cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=args.seed)

# Perform cross-validation with the best model
best_model = grid_search.best_estimator_
scores = cross_val_score(best_model, X, y, cv=kfold, scoring='accuracy')

# Print the cross-validation scores
print("Cross-Validation Scores:", scores)


exit(0)
"""

# Replace the per-model immediate-printing loop with a collection + unified reporting
models = [modelRF, modelDT, modelNB]
metrics = {}
predictions_test = {}

for model in models:
    name = model.__class__.__name__
    print(f"Training model: {name}")
    print(f"Training model with {len(X_train)} samples")
    model.fit(X_train, Y_train)

    print(f"Predicting {len(X_test)} samples")
    Y_pred = model.predict(X_test)
    predictions_test[name] = list(Y_pred)
    for i, y in enumerate(Y_pred):
        DataDict_test[i]["Y-pred"] = y

    # Quantized metrics (F1)
    f1_train_w = f1_score(Y_train, model.predict(X_train), average='weighted')
    f1_train_m = f1_score(Y_train, model.predict(X_train), average='micro')
    f1_train_M = f1_score(Y_train, model.predict(X_train), average='macro')

    f1_test_w = f1_score(Y_test, Y_pred, average='weighted')
    f1_test_m = f1_score(Y_test, Y_pred, average='micro')
    f1_test_M = f1_score(Y_test, Y_pred, average='macro')

    # Numerical metrics (Pearson on train GENIS, MAE on test)
    try:
        pearson_train = pearsonr([float(y) for y in Y_train], [float(y) for y in model.predict(X_train)])[0]
    except Exception:
        pearson_train = float('nan')
    try:
        mae_test = sklearn.metrics.mean_absolute_error([float(y) for y in Y_test], [float(y) for y in Y_pred])
    except Exception:
        mae_test = float('nan')

    # Feature importance where available
    feat_imp = None
    if hasattr(model, "feature_importances_"):
        feat_imp = list(model.feature_importances_)
    elif hasattr(model, "coef_"):
        coef = getattr(model, "coef_")
        # flatten linear model coef if necessary
        try:
            feat_imp = list(coef.ravel())
        except Exception:
            feat_imp = list(coef)

    metrics[name] = {
        "f1_train": (f1_train_w, f1_train_m, f1_train_M),
        "f1_test":  (f1_test_w,  f1_test_m,  f1_test_M),
        "pearson_train": pearson_train,
        "mae_test": mae_test,
        "feature_importance": feat_imp
    }

    # keep CSV write / model dumping behavior as before
    if (args.load is None or args.force):
        writeCSV(DataDict_train, DataDict_test, f'data/{model.__class__.__name__}_overall_results.csv')

    with open(f'data/{model.__class__.__name__}.pkl', 'wb') as model_file:
        pickle.dump(modelRF, model_file)
    print(f"Model dumped successfully in data/{model.__class__.__name__}.pkl (pickle format).")

# --- Unified aggregated reporting ---

# Baseline labels (already strings in earlier code)
V_labels = [str(v) for v in V_scores]
LLM_labels = [str(l) for l in LLM_scores]

def fmt(v):
    return f"{v:.2f}" if isinstance(v, float) and not (v != v) else "n/a"  # v!=v to detect NaN

# Quantized domain: one table with nine F1 columns
print("\nQuantized domain - aggregated F1 table")
hdr = [
    "Set",
    "F1-w_RF", "F1-w_DT", "F1-w_NB",
    "F1-m_RF", "F1-m_DT", "F1-m_NB",
    "F1-M_RF", "F1-M_DT", "F1-M_NB"
]
print("\t".join(hdr))

def get_f1(model_name, which, idx):
    m = metrics.get(model_name, {})
    vals = m.get(which)
    return fmt(vals[idx]) if vals is not None else "n/a"

# Train row
train_row = [
    "Train",
    get_f1("RandomForestClassifier", "f1_train", 0),
    get_f1("DecisionTreeClassifier", "f1_train", 0),
    get_f1("GaussianNB", "f1_train", 0),
    get_f1("RandomForestClassifier", "f1_train", 1),
    get_f1("DecisionTreeClassifier", "f1_train", 1),
    get_f1("GaussianNB", "f1_train", 1),
    get_f1("RandomForestClassifier", "f1_train", 2),
    get_f1("DecisionTreeClassifier", "f1_train", 2),
    get_f1("GaussianNB", "f1_train", 2),
]
print("\t".join(train_row))

# Test row
test_row = [
    "Test",
    get_f1("RandomForestClassifier", "f1_test", 0),
    get_f1("DecisionTreeClassifier", "f1_test", 0),
    get_f1("GaussianNB", "f1_test", 0),
    get_f1("RandomForestClassifier", "f1_test", 1),
    get_f1("DecisionTreeClassifier", "f1_test", 1),
    get_f1("GaussianNB", "f1_test", 1),
    get_f1("RandomForestClassifier", "f1_test", 2),
    get_f1("DecisionTreeClassifier", "f1_test", 2),
    get_f1("GaussianNB", "f1_test", 2),
]
print("\t".join(test_row))

# Baseline rows: compute weighted, micro and macro F1 separately for VADER and LLM
try:
    vader_f1_w = f1_score(Y_test, V_labels, average='weighted')
    vader_f1_m = f1_score(Y_test, V_labels, average='micro')
    vader_f1_M = f1_score(Y_test, V_labels, average='macro')
except Exception:
    vader_f1_w = vader_f1_m = vader_f1_M = float('nan')
try:
    llm_f1_w = f1_score(Y_test, LLM_labels, average='weighted')
    llm_f1_m = f1_score(Y_test, LLM_labels, average='micro')
    llm_f1_M = f1_score(Y_test, LLM_labels, average='macro')
except Exception:
    llm_f1_w = llm_f1_m = llm_f1_M = float('nan')

vader_row = ["VADER",
             fmt(vader_f1_w), fmt(vader_f1_w), fmt(vader_f1_w),
             fmt(vader_f1_m), fmt(vader_f1_m), fmt(vader_f1_m),
             fmt(vader_f1_M), fmt(vader_f1_M), fmt(vader_f1_M)]
llm_row = ["LLM",
           fmt(llm_f1_w), fmt(llm_f1_w), fmt(llm_f1_w),
           fmt(llm_f1_m), fmt(llm_f1_m), fmt(llm_f1_m),
           fmt(llm_f1_M), fmt(llm_f1_M), fmt(llm_f1_M)]
print("\t".join(vader_row))
print("\t".join(llm_row))

# Numerical domain: three columns for Pearson (RF,DT,NB) and three for MAE
print("\nNumerical domain - Pearson (train) and MAE (test)")
hdr2 = ["Set", "PearRF", "PearDT", "PearNB", "MAE_RF", "MAE_DT", "MAE_NB"]
print("\t".join(hdr2))

pearson_rf = metrics.get("RandomForestClassifier", {}).get("pearson_train", float('nan'))
pearson_dt = metrics.get("DecisionTreeClassifier", {}).get("pearson_train", float('nan'))
pearson_nb = metrics.get("GaussianNB", {}).get("pearson_train", float('nan'))

mae_rf = metrics.get("RandomForestClassifier", {}).get("mae_test", float('nan'))
mae_dt = metrics.get("DecisionTreeClassifier", {}).get("mae_test", float('nan'))
mae_nb = metrics.get("GaussianNB", {}).get("mae_test", float('nan'))

print("GENIS\t" + "\t".join(fmt(x) for x in [pearson_rf, pearson_dt, pearson_nb, mae_rf, mae_dt, mae_nb]))

# Baselines computed against numeric Y_test
Y_test_nums = [float(y) for y in Y_test]
try:
    vader_pearson = pearsonr(Y_test_nums, [float(v) for v in V_scores])[0]
    vader_mae = sklearn.metrics.mean_absolute_error(Y_test_nums, [float(v) for v in V_scores])
except Exception:
    vader_pearson = float('nan'); vader_mae = float('nan')
try:
    llm_pearson = pearsonr(Y_test_nums, [float(l) for l in LLM_scores])[0]
    llm_mae = sklearn.metrics.mean_absolute_error(Y_test_nums, [float(l) for l in LLM_scores])
except Exception:
    llm_pearson = float('nan'); llm_mae = float('nan')

# repeat baseline metrics across three classifier columns for easy column-wise comparison
print("VADER\t" + "\t".join(fmt(x) for x in [vader_pearson, vader_pearson, vader_pearson, vader_mae, vader_mae, vader_mae]))
print("LLM\t"   + "\t".join(fmt(x) for x in [llm_pearson, llm_pearson, llm_pearson, llm_mae, llm_mae, llm_mae]))

# Feature importance: row per feature, three classifier columns
print("\nFeature importance (per feature: RF, DT, NB)")
feat_hdr = ["Feature\t", "RF", "DT", "NB"]
print("\t".join(feat_hdr))
feature_names = ["O-score ", "G-scoreP", "G-scoreM", "G-scoreN"]
for idx, fname in enumerate(feature_names):
    rf_val = metrics.get("RandomForestClassifier", {}).get("feature_importance")
    dt_val = metrics.get("DecisionTreeClassifier", {}).get("feature_importance")
    nb_val = metrics.get("GaussianNB", {}).get("feature_importance")
    rf_str = f"{rf_val[idx]:.3f}" if rf_val and len(rf_val) > idx else "n/a"
    dt_str = f"{dt_val[idx]:.3f}" if dt_val and len(dt_val) > idx else "n/a"
    nb_str = f"{nb_val[idx]:.3f}" if nb_val and len(nb_val) > idx else "n/a"
    print("\t".join([fname, rf_str, dt_str, nb_str]))

# Confusion matrices: three GENIS matrices (RF, DT, NB) + LLM + VADER
if args.image:
    # prepare label set from numeric sources and produce ordered label strings
    label_nums = set()
    for y in Y_test:
        try:
            label_nums.add(float(y))
        except Exception:
            pass
    for v in V_scores:
        try:
            label_nums.add(float(v))
        except Exception:
            pass
    for l in LLM_scores:
        try:
            label_nums.add(float(l))
        except Exception:
            pass
    # add 0.5 and 10.5 so that the charts are not clipped
    label_nums.add(0.5)
    label_nums.add(10.5)
    ordered_labels = [f"{v:.1f}" for v in sorted(label_nums)]

    Y_test_fmt = [f"{float(y):.1f}" for y in Y_test]
    plt.figure(figsize=(15, 8))

    # GENIS confusion matrix for RandomForestClassifier
    plt.subplot(2, 3, 1)
    preds = predictions_test.get("RandomForestClassifier", [])
    preds_fmt = []
    for p in preds:
        try:
            preds_fmt.append(f"{float(p):.1f}")
        except Exception:
            preds_fmt.append(str(p))
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=preds_fmt,
        labels=ordered_labels,
        title=f"GENIS (RandomForestClassifier)",
        xlabel="GENIS",
        ylabel="Human"
    )

    # LLM
    plt.subplot(2, 3, 2)
    LLM_fmt = []
    for l in LLM_scores:
        try:
            LLM_fmt.append(f"{float(l):.1f}")
        except Exception:
            LLM_fmt.append(str(l))
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=LLM_fmt,
        labels=ordered_labels,
        title="LLM",
        xlabel="LLM",
        ylabel="Human"
    )

    # VADER
    plt.subplot(2, 3, 3)
    V_fmt = []
    for v in V_scores:
        try:
            V_fmt.append(f"{float(v):.1f}")
        except Exception:
            V_fmt.append(str(v))
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=V_fmt,
        labels=ordered_labels,
        title="VADER",
        xlabel="VADER",
        ylabel="Human"
    )

    for i, mname in enumerate(["DecisionTreeClassifier", "GaussianNB"], start=4):
        plt.subplot(2, 3, i)
        preds = predictions_test.get(mname, [])
        preds_fmt = []
        for p in preds:
            try:
                preds_fmt.append(f"{float(p):.1f}")
            except Exception:
                preds_fmt.append(str(p))
        plotConfusionMatrix(
            y_true=Y_test_fmt,
            y_pred=preds_fmt,
            labels=ordered_labels,
            title=f"GENIS ({mname})",
            xlabel="GENIS",
            ylabel="Human"
        )

    plt.subplot(2, 3, 6)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

print("Done.")
