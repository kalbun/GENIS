#
# GENIS - Grading with Explainability and No Intrinsic Saturation
#
# genisTrain.py - Script to train a linear regression model for sentiment analysis
# using features extracted from reviews.
# Before running this script, you should execute genisCalc.py to get the data needed.
#
import csv
import os
import argparse
import datetime
import pickle
from sklearn.model_selection import train_test_split
from scipy.stats import pearsonr
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error, r2_score, f1_score
from preprocessing import ReviewPreprocessor

ver: str = "0.10.0"
# Labels for the text and rating in the jsonl file
# The default values are the ones used in the Amazon reviews dataset
label_text: str = "text"
label_rating: str = "rating"

# Create an instance of class used in the script.
# Initialization postponed as it requires the cache path, calculated later.
preprocessor: ReviewPreprocessor = None

# Lists to store the data acquired from the csv files and the caches
Reviews: list[str] = []
X: list[list[float,float,float,float]] = []
Y: list[float] = []
V_scores: list[float] = []
LLM_scores: list[float] = []

helpMessage = """\
GENIS - Grading with Explainability and No Intrinsic Saturation

This script trains a linear regression model for sentiment analysis using features extracted from reviews.
To run, genisTrain requires a data file called selected_reviews.csv and the preprocessing cache generated by genisCalc.py.
Moreover, selected_reviews.csv must be modified beforehand to include the human scores for the reviews.

To locate these files, genisTrain uses two command line parameters:

- a positional parameter with the subdirectory of 'data/' where files to process are stored.
- the optional parameter -s or --seed is the random seed used with genisCalc.py (default 1967)

For example:

    python genisTrain.py music -s 42

This will look for the cache in data/music and the csv in data/music/42/selected_reviews.csv.
"""

print(f"GENIS trainer v{ver}")
parser = argparse.ArgumentParser(
    description=helpMessage,
    formatter_class=argparse.RawTextHelpFormatter
)
parser.add_argument(
    "paths",
    type=str,
    nargs="+",  # Accept one or more directories
    help="List of directories under 'data/' where processed files are stored"
)
parser.add_argument("-s", "--seed", type=int, help="Random seed (default 1967)", default=1967)
parser.add_argument("-v", "--version", action="version", version=f"{ver}")
args = parser.parse_args()

# Process command line arguments

for path in args.paths:

    cachePath = os.path.join("data", path)
    filePath = os.path.join(cachePath, f"{args.seed}", "selected_reviews.csv")
    if not os.path.exists(cachePath):
        print(f"Directory data/{path} not found.")
        exit(1)
    if not os.path.exists(filePath):
        print(f"File {filePath} not found.")
        exit(1)

    # Instantiate the ReviewPreprocessor class (which now handles cache initialization)
    preprocessor = ReviewPreprocessor(cachePath = cachePath)

    # Read the updated file with human scores
    counter: int = 0
    with open(filePath, mode='r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            review = row['review']
            hscore = float(row['hscore'])  # Convert human score to float

            # Retrieve data from the preprocessing cache
            cached_data = preprocessor.GetReviewFromCache(review)
            if cached_data:
                O_score = cached_data.get("score", 0)
                parsed_scores = cached_data.get("parsed_scores", {})
                # Extract the scores from the parsed scores dictionary
                L_scoreP = sum([score for score in parsed_scores.values() if score > 0])
                L_scoreM = sum([score for score in parsed_scores.values() if score < 0])
                L_scoreN = sum([score for score in parsed_scores.values() if score == 0])
                # Update the review dictionary with the LLM score
                V_score = cached_data.get("V-Whole", 0)
                LLM_score = cached_data.get("LLM-score", 0)
                # Append features to X and target to Y
                # Note that LLM score is not used in the model, but it is included for
                # later statistical analysis.
                X.append([O_score, L_scoreP, L_scoreM, L_scoreN])
                # Convert human score to string to force the regressor to
                # work to a classification problem instead of a regression one.
                Y.append(str(hscore))
                Reviews.append(cached_data.get("readable", ""))
                V_scores.append(V_score)
                LLM_scores.append(LLM_score)
                counter += 1

    print(f"Loaded {counter} reviews from {filePath}")

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, Y_train, Y_test, Rev_train, Rev_test, V_train, V_test, LLM_train, LLM_test = train_test_split(X, Y, Reviews, V_scores, LLM_scores, test_size=0.20, random_state=args.seed)

model = RandomForestClassifier(n_estimators=16, min_samples_split=5, random_state=args.seed)
print(f"Training model with {len(X_train)} samples")
model.fit(X_train, Y_train)
# Make predictions on the test set
print(f"Predicting {len(X_test)} samples")
Y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)
print("Evaluation metrics, F1 with human baseline:")
print("Params\tweight\tmicro\tmacro\tPearson")
print(f"Train\t{f1_score(Y_train, model.predict(X_train), average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='micro'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y_train], [float(y) for y in model.predict(X_train)])[0]:.2f}")
print(f"Test\t{f1_score(Y_test, Y_pred, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_test, Y_pred, average='micro'):.2f}",end="")  
print(f"\t{f1_score(Y_test, Y_pred, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y_test], [float(y) for y in Y_pred])[0]:.2f}")
V_labels = [ str(int(( (v + 1) * 3.5 + 1) * 2) / 2) for v in V_scores]  # Adjust VADER scores to match the labels
print(f"VADER\t{f1_score(Y, V_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y, V_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y, V_labels, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y], [float(y) for y in V_scores])[0]:.2f}")
LLM_labels = [ str(l) for l in LLM_scores]  # Adjust LLM scores to match the labels
print(f"LLM\t{f1_score(Y,LLM_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y, LLM_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y, LLM_labels, average='macro'):.2f}",end="")
print(f"\t{pearsonr([float(y) for y in Y], [float(y) for y in LLM_scores])[0]:.2f}")

# Write the contents of all files into a new CSV file
with open('data/overall_results.csv', 'w', encoding='utf-8', newline='') as file:
    writer = csv.writer(file)
    # Write the header
    writer.writerow([
        "timestamp",  # timestamp
        "type",  # type (train/test)
        "O-score",  # O-score
        "L-scoreP",  # L-scoreP
        "L-scoreM",  # L-scoreM
        "L-scoreN",  # L-scoreN
        "Y",  # Y (human score)
        "Y-pred",  # Y-pred (to be filled later)
        "VADER",  # VADER score
        "LLM",  # LLM score
        "review"  # review file
    ])
    for x, y, v, llm, review in zip(X_train, Y_train, V_train, LLM_train, Rev_train):
        writer.writerow([
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # timestamp
            "tr",  # type (train/test)
            x[0],  # O-score
            x[1],  # L-scoreP
            x[2],  # L-scoreM
            x[3],  # L-scoreN
            float(y),  # Y (human score)
            float(y),  # Y-pred (same as Y for training data)
            v,  # VADER score
            llm,  # LLM score
            review,  # review file
        ])
    for x, y, y_pred, v, llm, review in zip(X_test, Y_test, Y_pred, V_test, LLM_test, Rev_test):
        writer.writerow([
            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # timestamp
            "te",  # type (train/test)
            x[0],  # O-score
            x[1],  # L-scoreP
            x[2],  # L-scoreM
            x[3],  # L-scoreN
            float(y),  # Y (human score)
            float(y_pred),  # Y-pred (to be filled later)
            v,  # VADER score
            llm,  # LLM score
            review,  # review file
        ])
file.close()
print("Results written to data/overall_results.csv")

# Save the model to a file
with open('data/random_forest_classifier.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
print("Model dumped successfully in data/random_forest_classifier.pkl (pickle format).")
print("Done.")
