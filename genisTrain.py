#
# GENIS - Grading with Explainability and No Intrinsic Saturation
#
# genisTrain.py - Script to train a linear regression model for sentiment analysis
# using features extracted from reviews.
# Before running this script, you should execute genisCalc.py to get the data needed.
#
import csv
import os
import argparse
import datetime
import pickle
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr, pearsonr
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
import sklearn.metrics
from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score
from preprocessing import ReviewPreprocessor
import matplotlib.pyplot as plt

ver: str = "0.12.0"
# Labels for the text and rating in the jsonl file
# The default values are the ones used in the Amazon reviews dataset
label_text: str = "text"
label_rating: str = "rating"

# Create an instance of class used in the script.
# Initialization postponed as it requires the cache path, calculated later.
preprocessor: ReviewPreprocessor = None

# Dictionary to store the data acquired from the csv files and the caches
DataDict: dict = {}
# Lists to store the data acquired from the csv files and the caches
Reviews: list[str] = []
X_train: list[list[float]] = []
X_test: list[list[float]] = []
Y: list[float] = []
Y_train: list[float] = []
Y_test: list[float] = []
V_scores: list[float] = []
LLM_scores: list[float] = []

helpMessage = """\
GENIS - Grading with Explainability and No Intrinsic Saturation

This script trains a linear regression model for sentiment analysis using features extracted from reviews.
To run, genisTrain requires a data file called selected_reviews.csv and the preprocessing cache generated by genisCalc.py.
Moreover, selected_reviews.csv must be modified beforehand to include the human scores for the reviews.

To locate these files, genisTrain uses two command line parameters:

- a positional parameter with the subdirectory of 'data/' where files to process are stored.
- the optional parameter -s or --seed is the random seed used with genisCalc.py (default 1967)

For example:

    python genisTrain.py music -s 42

This will look for the cache in data/music and the csv in data/music/42/selected_reviews.csv.
"""

def writeCSV(train_data, test_data, filename):
    """Write the overall results CSV from training and test data dictionaries."""
    with open(filename, 'w', encoding='utf-8', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([
            "timestamp", "type", "O-score", "G-scoreP", "G-scoreM", "G-scoreN", "Y", "Y-pred", "V-converted", "LLM", "VADER", "review", "filename"
        ])
        for data in train_data:
            writer.writerow([
                datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "tr",
                data["O-score"],
                data["G-scoreP"],
                data["G-scoreM"],
                data["G-scoreN"],
                float(data["Y"]),
                float(data["Y"]),
                data["V-converted"],
                data["LLM-score"],
                data["VADER"],
                data["review"],
                data["filename"]
            ])
        for data in test_data:
            writer.writerow([
                datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "te",
                data["O-score"],
                data["G-scoreP"],
                data["G-scoreM"],
                data["G-scoreN"],
                float(data["Y"]),
                float(data["Y-pred"]),
                data["V-converted"],
                data["LLM-score"],
                data["VADER"],
                data["review"],
                data["filename"]
            ])
    print(f"Results written to {filename}")

def loadCSV(filename: str) -> dict:
    """Load the overall results CSV and return lists for each column."""
    timestamps, types, O_scores, G_scorePs, G_scoreMs, G_scoreNs = [], [], [], [], [], []
    Ys, Y_preds, V_converted, LLMs, VADERs, reviews, filenames = [], [], [], [], [], [], []

    dictionary: dict[str, dict] = {}

    with open(filename, 'r', encoding='utf-8') as file:
        fieldnames = [
            "timestamp", "type", "O-score", "G-scoreP", "G-scoreM", "G-scoreN", "Y", "Y-pred", "V-converted", "LLM", "VADER", "review", "filename"
        ]
        # create a reader. The first line is the header
        # and the rest is the data
        reader = csv.DictReader(file, fieldnames=fieldnames)
        reader.__next__()  # Skip the header
        for row in reader:
            # Append each row to the corresponding list
            # Note that the LM model is a classifier, so we need to convert
            # the score to a string to match the labels
            dictionary[row["review"]] = {
                "timestamp": row["timestamp"],
                "type": row["type"],
                "O-score": float(row["O-score"]),
                "G-scoreP": float(row["G-scoreP"]),
                "G-scoreM": float(row["G-scoreM"]),
                "G-scoreN": float(row["G-scoreN"]),
                "Y": str(row["Y"]),
                "Y-pred": str(row["Y-pred"]),
                "V-converted": str(row["V-converted"]),
                "LLM-score": str(row["LLM"]),
                "VADER": float(row["VADER"]),
                "review": row["review"],
                "filename": row["filename"]
            }
        return dictionary

def plotConfusionMatrix(y_true: list, y_pred: list, labels: list, title: str, xlabel: str, ylabel: str):
    """Plot the confusion matrix."""
    cm = confusion_matrix(y_true, y_pred, labels=labels)
#    plt.imshow(cm, cmap=plt.cm.Grays)
#    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Grays)
#    plt.colorbar()
    # Set the title and labels
    plt.title(title)
    tick_marks = range(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.xlabel(xlabel=xlabel)
    plt.ylabel(ylabel=ylabel)
    # Annotate the cells with the confusion matrix values
    max_value = cm.max()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            if (i == j and i >= 1 and i < 20):
#                plt.gca().add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='lime', lw=0.5))
                plt.gca().add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, color='lime', alpha=0.3))
            if cm[i, j] != 0:
                if (i != j):
                    plt.gca().add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=True, color='red', alpha=0.2))
                plt.text(j, i, format(cm[i, j], 'd'),ha="center", va="center", color='black', fontsize=10)
                # also set background color


DataDict: dict = {}

print(f"GENIS trainer v{ver}")
parser = argparse.ArgumentParser(
    description=helpMessage,
    formatter_class=argparse.RawTextHelpFormatter
)
parser.add_argument(
    "paths",
    type=str,
    nargs="*",  # Accept zero or more directories
    help="List of directories under 'data/' where processed files are stored"
)
parser.add_argument("-s", "--seed", type=int, help="Random seed (default 1967)", default=1967)
parser.add_argument("-v", "--version", action="version", version=f"{ver}")
parser.add_argument("-i", "--image", action="store_true", help="Generate confusion matrix images")
parser.add_argument('-l', '--load', type=str, default=None, help="Load data from a CSV file instead of reading the paths")
parser.add_argument('-f', '--force', action='store_true', help="Force overwrite of existing files")
args = parser.parse_args()

# Process command line arguments

if (args.load is not None):
    # Load data from the specified CSV file
    if not os.path.exists(args.load):
        print(f"File {args.load} not found.")
        exit(1)
    # Load the CSV file into DataDict
    DataDict = loadCSV(args.load)
    print(f"Loaded {len(DataDict)} reviews from {args.load}")

elif len(args.paths) != 0:

    for path in args.paths:

        cachePath = os.path.join("data", path)
        filePath = os.path.join(cachePath, f"{args.seed}", "selected_reviews.csv")
        if not os.path.exists(cachePath):
            print(f"Directory data/{path} not found.")
            exit(1)
        if not os.path.exists(filePath):
            print(f"File {filePath} not found.")
            exit(1)

        # Instantiate the ReviewPreprocessor class (which now handles cache initialization)
        preprocessor = ReviewPreprocessor(cachePath = cachePath)

        # Read the updated file with human scores
        counter: int = 0
        with open(filePath, mode='r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                review = row['review']
                Y = float(row['hscore'])  # Convert human score to float

                # Retrieve data from the preprocessing cache
                cached_data = preprocessor.GetReviewFromCache(review)
                if cached_data:
                    O_score = cached_data.get("score", 0)
                    parsed_scores = cached_data.get("parsed_scores", {})
                    # Extract the scores from the parsed scores dictionary
                    DataDict[review] = {
                        "O-score": O_score,
                        "G-scoreP": sum([score for score in parsed_scores.values() if score > 0]),
                        "G-scoreM": sum([score for score in parsed_scores.values() if score < 0]),
                        "G-scoreN": sum([score for score in parsed_scores.values() if score == 0]),
                        "Y": str(Y),
                        "Y-pred": str(Y),  # Placeholder for predictions
                        "VADER": cached_data.get("V-Whole", 0),
                        # convert VADER score to a 1-10 scale with half grades
                        "V-converted": round( ((cached_data.get("V-Whole", 0) + 1) * 4.5 + 1) * 2, 0) / 2,
                        "LLM-score": cached_data.get("LLM-score", 0),
                        "readable": cached_data.get("readable", ""),
                        "filename": path
                    }
                    counter += 1

        print(f"Loaded {counter} reviews from {filePath}")

else:
    print("Please provide a list of directories or use the -l option to load data from a CSV file.")
    exit(1)

# At this point, DataDict contains all the data needed for training, obtained
# either from the paths provided or from the CSV file.

# Filter out reviews with no scores
#DataDict = {key: values for key, values in DataDict.items() if (values["G-scoreP"] + values["G-scoreM"] + values["G-scoreN"]) > 0}

# Split the data into training and testing sets (80% train, 20% test)
DataDict_train, DataDict_test = train_test_split(list(DataDict.values()), test_size=0.2, random_state=args.seed)

Y = [data["Y"] for data in DataDict.values()]
X_train = [
    [data["O-score"], data["G-scoreP"], data["G-scoreM"], data["G-scoreN"]
     ]
    for data in DataDict_train
    ]
X_test = [
    [data["O-score"], data["G-scoreP"], data["G-scoreM"], data["G-scoreN"]
     ]
    for data in DataDict_test
    ]
Y_train = [data["Y"] for data in DataDict_train]
Y_test = [data["Y"] for data in DataDict_test]
V_scores = [data["V-converted"] for data in DataDict.values()]
V_scores = [data["V-converted"] for data in DataDict_test]
V_labels = [str(v) for v in V_scores]
LLM_scores = [data["LLM-score"] for data in DataDict.values()]
LLM_scores = [data["LLM-score"] for data in DataDict_test]
LLM_labels = [ str(l) for l in LLM_scores]  # Adjust LLM scores to match the labels

#model = RandomForestClassifier(n_estimators=32, min_samples_split=3, random_state=args.seed)
model = RandomForestClassifier(
    n_estimators=96,
    max_depth=10,
    max_leaf_nodes=25,
    min_samples_leaf=2,
    random_state=args.seed)

"""
model = DecisionTreeClassifier(
    criterion='gini',
    max_depth=10,
    min_samples_split=2,
    max_leaf_nodes=20,
    min_samples_leaf=1,
    random_state=args.seed)
"""

"""
# Random forest calibration

from sklearn.model_selection import cross_val_score, KFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import numpy as np


X = X_train + X_test
y = Y_train + Y_test
# Convert the lists to numpy arrays for compatibility with scikit-learn
X = np.array(X)
y = np.array(y)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [x for x in range(48, 129, 8)],
    'criterion': ['gini'],
    'max_depth': [None, 10, 20],
    'min_samples_split': [x for x in range(2, 4)],
    'max_leaf_nodes': [None, 15, 20, 25],
    'min_samples_leaf': [1, 2],
    'max_features': ['log2']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Perform grid search
grid_search.fit(X, y)

# Print the best parameters and the best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# Initialize k-fold cross-validation
kfold = KFold(n_splits=5, shuffle=True, random_state=args.seed)

# Perform cross-validation with the best model
best_model = grid_search.best_estimator_
scores = cross_val_score(best_model, X, y, cv=kfold, scoring='accuracy')

# Print the cross-validation scores
print("Cross-Validation Scores:", scores)


exit(0)
#"""

print(f"Training model with {len(X_train)} samples")
model.fit(X_train, Y_train)
# Make predictions on the test set
print(f"Predicting {len(X_test)} samples")
Y_pred = model.predict(X_test)
for i, y in enumerate(Y_pred):
    DataDict_test[i]["Y-pred"] = y

# Evaluate the model
print("Evaluation metrics, baseline = human grades:")
print("\n***Quantized domain (scores as labels)")
print("Set\tF1-w\tF1-m\tF1-M")
print(f"Train\t{f1_score(Y_train, model.predict(X_train), average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='micro'):.2f}",end="")
print(f"\t{f1_score(Y_train, model.predict(X_train), average='macro'):.2f}")
print(f"Test\t{f1_score(Y_test, Y_pred, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_test, Y_pred, average='micro'):.2f}",end="")  
print(f"\t{f1_score(Y_test, Y_pred, average='macro'):.2f}")
print(f"VADER\t{f1_score(Y_test, V_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_test, V_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y_test, V_labels, average='macro'):.2f}")
print(f"LLM\t{f1_score(Y_test,LLM_labels, average='weighted'):.2f}",end="")
print(f"\t{f1_score(Y_test, LLM_labels, average='micro'):.2f}",end="")
print(f"\t{f1_score(Y_test, LLM_labels, average='macro'):.2f}")

# calculate MAE
print("\n***Numerical domain (scores as numbers)")
print("Set\tPearson\tMAE")
print(f"GENIS\t{pearsonr([float(y) for y in Y_train], [float(y) for y in model.predict(X_train)])[0]:.2f}",end="\t")
print(sklearn.metrics.mean_absolute_error([float(y) for y in Y_test], [float(y) for y in Y_pred]))
print(f"VADER\t{pearsonr([float(y) for y in Y_test], [float(y) for y in V_scores])[0]:.2f}",end="\t")
print(sklearn.metrics.mean_absolute_error([float(y) for y in Y_test], [float(y) for y in V_labels]))
print(f"LLM\t{pearsonr([float(y) for y in Y_test], [float(y) for y in LLM_scores])[0]:.2f}",end="\t")
print(sklearn.metrics.mean_absolute_error([float(y) for y in Y_test], [float(y) for y in LLM_labels]))

feature_names = ["O-score", "G-scoreP", "G-scoreM", "G-scoreN"]
print("Feature importance (Random Forest):")
for i, feature in enumerate(feature_names):
    print(f"{feature}: {model.feature_importances_[i]:.4f}")

Y_test_fmt = [f"{float(y):.1f}" for y in Y_test]
Y_pred_fmt = [f"{float(y):.1f}" for y in Y_pred]
LLM_labelsT = [f"{float(data["LLM-score"]):.1f}" for data in DataDict_test]
V_labels_fmt = [f"{float(v):.1f}" for v in V_labels]

if (args.image):
    # Show the confusion matrices
    ordered_labels = [f"{(v/2):.1f}" for v in range(2, 21)]  # Assuming the labels are from 1 to 10
    ordered_labels = [f"{(v/2):.1f}" for v in range(1, 22)]  # Assuming the labels are from 1 to 10

    plt.figure(figsize=(10, 7))
    plt.subplot(1, 3, 1)
    # Y_pred vs Y_test
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=Y_pred_fmt,
        labels=ordered_labels,
        title="Confusion matrix for GENIS",
        xlabel="GENIS",
        ylabel="Human"
    )


    plt.subplot(1, 3, 2)
    # Calculate ordered labels from numeric values. This quite complex operation is
    # needed to show the labels in numerical order.
    # Only get the labels from the test set
#    LLM_labelsT = [str(l) for l in [data["LLM-score"] for data in DataDict_test]]
    # LLM vs Y_test
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=LLM_labelsT,
        labels=ordered_labels,
        title="Confusion matrix for LLM",
        xlabel="LLM",
        ylabel="Human"
    )

    plt.subplot(1, 3, 3)
    # Third confusion matrix (VADER vs Y_test)
    plotConfusionMatrix(
        y_true=Y_test_fmt,
        y_pred=V_labels_fmt,
        labels=ordered_labels,
        title="Confusion matrix for VADER",
        xlabel="VADER",
        ylabel="Human"
    )

#    plt.suptitle("Confusion matrices")  
    plt.tight_layout()
    plt.show()

# Write the contents of all files into a new CSV file
if (args.load is None or args.force):
    writeCSV(DataDict_train, DataDict_test, 'data/overall_results.csv')

# Save the model to a file
with open('data/random_forest_classifier.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
print("Model dumped successfully in data/random_forest_classifier.pkl (pickle format).")
print("Done.")
